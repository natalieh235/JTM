{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cpc_training_exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS3zre3hVRaa",
        "outputId": "34e4e4ba-7042-439b-c7f8-95a738387ef5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/DLProjects/JTM"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/DLProjects/JTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmv_AG1ws6Sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09daa69e-b907-4737-9148-8133651b4817"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-e320770a-9513-9d83-c97a-5b28b37584ff)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4W0oTBeW8q_",
        "outputId": "87a4327e-a59b-404a-f008-a4006817ffe0"
      },
      "source": [
        "!pip install comet_ml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting comet_ml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/5a/50260282d44740a757d17829e9c6c4a7a90430f2c484bacbdcc31c6a4ab3/comet_ml-3.10.0-py2.py3-none-any.whl (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (7.352.0)\n",
            "Collecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/ee/7aa724dc2dbed9b028f463eada5482770c13b7381a0c79457d12b3b62de2/websocket_client-1.0.1-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.23.0)\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ac/b7082c3d228e600af37ec5cf99697d400328b13350b4d7577c213fa4faca/wurlitzer-2.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.6.0)\n",
            "Collecting requests-toolbelt>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hCollecting dulwich>=0.20.6; python_version >= \"3.0\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/71/cb573395e780b1d0406a632e6904d3e933f3a80ea4386d5ae2f3be41d00b/dulwich-0.20.22.tar.gz (414kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.12.1)\n",
            "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/dc/38593280ec30fe1cb2611ec65554b76b68d13582bf490113e3332cdd85ea/everett-1.0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Collecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Building wheels for collected packages: dulwich, configobj\n",
            "  Building wheel for dulwich (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dulwich: filename=dulwich-0.20.22-cp37-cp37m-linux_x86_64.whl size=525735 sha256=b45dccd3bb4a303774000c99f3c5fe3c0b8bbd3644f761bfba1eef18b945819f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/e2/e1/cf31410f7792536bac3bab35f40d30d2b87183220229172031\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp37-none-any.whl size=34547 sha256=48e34d8e172525118473899ca62bac2f075b14ed74bf099fc191b423a4baf2dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built dulwich configobj\n",
            "Installing collected packages: websocket-client, wurlitzer, requests-toolbelt, dulwich, configobj, everett, comet-ml\n",
            "Successfully installed comet-ml-3.10.0 configobj-5.0.6 dulwich-0.20.22 everett-1.0.3 requests-toolbelt-0.9.1 websocket-client-1.0.1 wurlitzer-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so0Ie8ZO5TVd",
        "outputId": "d07c873b-1c7f-4527-fbf8-9afe28bdcac5"
      },
      "source": [
        "from comet_ml import Experiment\n",
        "\n",
        "experiment = Experiment(\n",
        "    api_key=\"K5CvVquVZJNg9xfY2ip95FuoD\",\n",
        "    project_name=\"jtm\",\n",
        "    workspace=\"tiagocuervo\"\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/tiagocuervo/jtm/e20b0c4e435043a3905ad1fc4b1e3e89\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0pN8vB4qV6"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w4ZEQ4r4s5p"
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import Sampler, BatchSampler\n",
        "import librosa\n",
        "import tqdm\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AudioBatchData(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 rawAudioPath,\n",
        "                 metadataPath,\n",
        "                 sizeWindow,\n",
        "                 labelsBy='composer',\n",
        "                 outputPath=None,\n",
        "                 CHUNK_SIZE=1e9,\n",
        "                 NUM_CHUNKS_INMEM=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            - rawAudioPath (string): path to the raw audio files\n",
        "            - metadataPath (string): path to the data set metadata (used to define labels)\n",
        "            - sizeWindow (int): size of the sliding window\n",
        "            - labelsBy (string): name of column in metadata according to which create labels\n",
        "            - outputPath (string): path to the directory where chunks are to be created or are stored\n",
        "            - CHUNK_SIZE (int): desired size in bytes of a chunk\n",
        "            - NUM_CHUNKS_INMEM (int): target maximal size chunks of data to load in memory at a time\n",
        "        \"\"\"\n",
        "        self.NUM_CHUNKS_INMEM = NUM_CHUNKS_INMEM\n",
        "        self.CHUNK_SIZE = CHUNK_SIZE\n",
        "        self.rawAudioPath = Path(rawAudioPath)\n",
        "        self.sizeWindow = sizeWindow\n",
        "\n",
        "        self.sequencesData = pd.read_csv(metadataPath, index_col='id')\n",
        "        self.sequencesData = self.sequencesData.sort_values(by=labelsBy)\n",
        "        self.sequencesData[labelsBy] = self.sequencesData[labelsBy].astype('category')\n",
        "        self.sequencesData[labelsBy] = self.sequencesData[labelsBy].cat.codes\n",
        "\n",
        "        self.totSize = self.sequencesData['length'].sum()\n",
        "        # print(\"Total size:\", self.totSize)\n",
        "        # print(\"Length of data set:\", self.__len__())\n",
        "\n",
        "        self.category = labelsBy\n",
        "\n",
        "        if outputPath is None:\n",
        "            self.chunksDir = self.rawAudioPath / labelsBy\n",
        "        else:\n",
        "            self.chunksDir = Path(outputPath) / labelsBy\n",
        "\n",
        "        if not os.path.exists(self.chunksDir):\n",
        "            os.makedirs(self.chunksDir)\n",
        "\n",
        "        packages2Load = [fileName for fileName in os.listdir(self.chunksDir) if\n",
        "                         re.match(r'chunk_.*[0-9]+.pickle', fileName)]\n",
        "\n",
        "        if len(packages2Load) == 0:\n",
        "            self._createChunks()\n",
        "            packages2Load = [fileName for fileName in os.listdir(self.chunksDir) if\n",
        "                             re.match(r'chunk_.*[0-9]+.pickle', fileName)]\n",
        "        else:\n",
        "            print(\"Chunks already exist at\", self.chunksDir)\n",
        "\n",
        "        self.packs = []\n",
        "        packOfChunks = []\n",
        "        for i, packagePath in enumerate(packages2Load):\n",
        "            packOfChunks.append(packagePath)\n",
        "            if (i + 1) % self.NUM_CHUNKS_INMEM == 0:\n",
        "                self.packs.append(packOfChunks)\n",
        "                packOfChunks = []\n",
        "        if len(packOfChunks) > 0:\n",
        "            self.packs.append(packOfChunks)\n",
        "\n",
        "        self.currentPack = -1\n",
        "        self.nextPack = 0\n",
        "        self.sequenceIdx = 0\n",
        "\n",
        "        self.data = None\n",
        "\n",
        "        self._loadNextPack(first=True)\n",
        "        self._loadNextPack()\n",
        "\n",
        "    def _createChunks(self):\n",
        "        print(\"Creating chunks at\", self.chunksDir)\n",
        "        pack = []\n",
        "        packIds = []\n",
        "        packageSize = 0\n",
        "        packageIdx = 0\n",
        "        for trackId in tqdm.tqdm(self.sequencesData.index):\n",
        "            sequence, samplingRate = librosa.load(self.rawAudioPath / (str(trackId) + '.wav'), sr=16000)\n",
        "            sequence = torch.tensor(sequence).float()\n",
        "            packIds.append(trackId)\n",
        "            pack.append(sequence)\n",
        "            packageSize += len(sequence) * 4\n",
        "            if packageSize >= self.CHUNK_SIZE:\n",
        "                print(f\"Saved pack {packageIdx}\")\n",
        "                with open(self.chunksDir / f'chunk_{packageIdx}.pickle', 'wb') as handle:\n",
        "                    pickle.dump(torch.cat(pack, dim=0), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                with open(self.chunksDir / f'ids_{packageIdx}.pickle', 'wb') as handle:\n",
        "                    pickle.dump(packIds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                pack = []\n",
        "                packIds = []\n",
        "                packageSize = 0\n",
        "                packageIdx += 1\n",
        "        print(f\"Saved pack {packageIdx}\")\n",
        "        with open(self.chunksDir / f'chunk_{packageIdx}.pickle', 'wb') as handle:\n",
        "            pickle.dump(torch.cat(pack, dim=0), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.chunksDir / f'ids_{packageIdx}.pickle', 'wb') as handle:\n",
        "            pickle.dump(packIds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def _loadNextPack(self, first=False):\n",
        "        self.clear()\n",
        "        if not first:\n",
        "            self.currentPack = self.nextPack\n",
        "            startTime = time.time()\n",
        "            print('Loading files')\n",
        "            self.categoryLabel = [0]\n",
        "            packageIdx = [0]\n",
        "            self.seqLabel = [0]\n",
        "            packageSize = 0\n",
        "            previousCategory = 0\n",
        "            for packagePath in self.packs[self.currentPack]:\n",
        "                with open(self.chunksDir / ('ids_' + packagePath.split('_', maxsplit=1)[-1]), 'rb') as handle:\n",
        "                    chunkIds = pickle.load(handle)\n",
        "                for seqId in chunkIds:\n",
        "                    currentCategory = self.sequencesData.loc[seqId][self.category]\n",
        "                    if currentCategory != previousCategory:\n",
        "                        self.categoryLabel.append(packageSize)\n",
        "                    previousCategory = currentCategory\n",
        "                    packageSize += self.sequencesData.loc[seqId].length\n",
        "                    self.seqLabel.append(packageSize)\n",
        "                packageIdx.append(packageSize)\n",
        "\n",
        "            self.data = torch.empty(size=(packageSize,))\n",
        "            for i, packagePath in enumerate(self.packs[self.currentPack]):\n",
        "                with open(self.chunksDir / packagePath, 'rb') as handle:\n",
        "                    self.data[packageIdx[i]:packageIdx[i + 1]] = pickle.load(handle)\n",
        "            print(f'Loaded {len(self.seqLabel) - 1} sequences, elapsed={time.time() - startTime:.3f} secs')\n",
        "\n",
        "        self.nextPack = (self.currentPack + 1) % len(self.packs)\n",
        "        if self.nextPack == 0 and len(self.packs) > 1:\n",
        "            self.currentPack = -1\n",
        "            self.nextPack = 0\n",
        "            self.sequenceIdx = 0\n",
        "\n",
        "    def clear(self):\n",
        "        if 'data' in self.__dict__:\n",
        "            del self.data\n",
        "        if 'categoryLabel' in self.__dict__:\n",
        "            del self.categoryLabel\n",
        "        if 'seqLabel' in self.__dict__:\n",
        "            del self.seqLabel\n",
        "\n",
        "    def getCategoryLabel(self, idx):\n",
        "        idCategory = next(x[0] for x in enumerate(self.categoryLabel) if x[1] > idx) - 1\n",
        "        return idCategory\n",
        "\n",
        "    def getSequenceLabel(self, idx):\n",
        "        return self.categoryLabel[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.totSize // self.sizeWindow\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < 0 or idx >= len(self.data) - self.sizeWindow - 1:\n",
        "            print(idx)\n",
        "\n",
        "        outData = self.data[idx:(self.sizeWindow + idx)].view(1, -1)\n",
        "        label = torch.tensor(self.getCategoryLabel(idx), dtype=torch.long)\n",
        "        return outData, label\n",
        "\n",
        "    def getBaseSampler(self, samplingType, batchSize, offset):\n",
        "        if samplingType == \"samecategory\":\n",
        "            return SameTrackSampler(batchSize, self.categoryLabel, self.sizeWindow, offset)\n",
        "        if samplingType == \"samesequence\":\n",
        "            return SameTrackSampler(batchSize, self.seqLabel, self.sizeWindow, offset)\n",
        "        if samplingType == \"sequential\":\n",
        "            return SequentialSampler(len(self.data), self.sizeWindow, offset, batchSize)\n",
        "\n",
        "        sampler = UniformAudioSampler(len(self.data), self.sizeWindow, offset)\n",
        "        return BatchSampler(sampler, batchSize, True)\n",
        "\n",
        "    def getDataLoader(self, batchSize, samplingType, randomOffset, numWorkers=0,\n",
        "                      onLoop=-1):\n",
        "        r\"\"\"\n",
        "        Get a batch sampler for the current dataset.\n",
        "            - batchSize (int): batch size\n",
        "            - groupSize (int): in the case of type in [\"track\", \"sequence\"]\n",
        "            number of items sharing a same label in the group\n",
        "            (see AudioBatchSampler)\n",
        "            - type (string):\n",
        "                type == \"track\": grouped sampler track-wise\n",
        "                type == \"sequence\": grouped sampler sequence-wise\n",
        "                type == \"sequential\": sequential sampling\n",
        "                else: uniform random sampling of the full audio\n",
        "                vector\n",
        "            - randomOffset (bool): if True add a random offset to the sampler\n",
        "                                   at the begining of each iteration\n",
        "        \"\"\"\n",
        "        nLoops = len(self.packs)\n",
        "        totSize = self.totSize // (self.sizeWindow * batchSize)\n",
        "        if onLoop >= 0:\n",
        "            self.currentPack = onLoop - 1\n",
        "            self._loadNextPack()\n",
        "            nLoops = 1\n",
        "\n",
        "        def samplerCall():\n",
        "            offset = random.randint(0, self.sizeWindow // 2) \\\n",
        "                if randomOffset else 0\n",
        "            return self.getBaseSampler(samplingType, batchSize, offset)\n",
        "\n",
        "        return AudioLoader(self, samplerCall, nLoops, self._loadNextPack, totSize, numWorkers)\n",
        "\n",
        "\n",
        "class AudioLoader(object):\n",
        "    r\"\"\"\n",
        "    A DataLoader meant to handle an AudioBatchData object.\n",
        "    In order to handle big datasets AudioBatchData works with big chunks of\n",
        "    audio it loads sequentially in memory: once all batches have been sampled\n",
        "    on a chunk, the AudioBatchData loads the next one.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 samplerCall,\n",
        "                 nLoop,\n",
        "                 updateCall,\n",
        "                 size,\n",
        "                 numWorkers):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            - dataset (AudioBatchData): target dataset\n",
        "            - samplerCall (function): batch-sampler to call\n",
        "            - nLoop (int): number of chunks to load\n",
        "            - updateCall (function): function loading the next chunk\n",
        "            - size (int): total number of batches\n",
        "            - numWorkers (int): see torch.utils.data.DataLoader\n",
        "        \"\"\"\n",
        "        self.samplerCall = samplerCall\n",
        "        self.updateCall = updateCall\n",
        "        self.nLoop = nLoop\n",
        "        self.size = size\n",
        "        self.dataset = dataset\n",
        "        self.numWorkers = numWorkers\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        for i in range(self.nLoop):\n",
        "            sampler = self.samplerCall()\n",
        "            dataloader = DataLoader(self.dataset,\n",
        "                                    batch_sampler=sampler,\n",
        "                                    num_workers=self.numWorkers)\n",
        "            # print(\"Data loader nLoop: \", self.nLoop)\n",
        "            # print(\"Len data loader: \", len(dataloader))\n",
        "            # print(\"Len of sampler: \", len(sampler))\n",
        "            # assert False\n",
        "            # print(\"Dataloader len: \\n\", len(dataloader))\n",
        "            for j, x in enumerate(dataloader):\n",
        "                # print(\"Data loader yielded batch #: \", j)\n",
        "                yield x\n",
        "            # print(\"Len data loader: \", len(dataloader), \"and consummed: \", j + 1)\n",
        "            if i < self.nLoop - 1:\n",
        "                self.updateCall()\n",
        "\n",
        "\n",
        "class UniformAudioSampler(Sampler):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataSize,\n",
        "                 sizeWindow,\n",
        "                 offset):\n",
        "        self.len = dataSize // sizeWindow\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.offset = offset\n",
        "        if self.offset > 0:\n",
        "            self.len -= 1\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter((self.offset\n",
        "                     + self.sizeWindow * torch.randperm(self.len)).tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class SequentialSampler(Sampler):\n",
        "\n",
        "    def __init__(self, dataSize, sizeWindow, offset, batchSize):\n",
        "\n",
        "        self.len = (dataSize // sizeWindow) // batchSize\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.offset = offset\n",
        "        self.startBatches = [x * (dataSize // batchSize)\n",
        "                             for x in range(batchSize)]\n",
        "        self.batchSize = batchSize\n",
        "        if self.offset > 0:\n",
        "            self.len -= 1\n",
        "\n",
        "    def __iter__(self):\n",
        "        for idx in range(self.len):\n",
        "            yield [self.offset + self.sizeWindow * idx\n",
        "                   + start for start in self.startBatches]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class SameTrackSampler(Sampler):\n",
        "\n",
        "    def __init__(self,\n",
        "                 batchSize,\n",
        "                 samplingIntervals,\n",
        "                 sizeWindow,\n",
        "                 offset):\n",
        "\n",
        "        self.samplingIntervals = samplingIntervals\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.batchSize = batchSize\n",
        "        self.offset = offset\n",
        "\n",
        "        if self.samplingIntervals[0] != 0:\n",
        "            raise AttributeError(\"Sampling intervals should start at zero\")\n",
        "\n",
        "        nWindows = len(self.samplingIntervals) - 1\n",
        "        self.sizeSamplers = [(self.samplingIntervals[i + 1] -\n",
        "                              self.samplingIntervals[i]) // self.sizeWindow\n",
        "                             for i in range(nWindows)]  # How many windows a sequence/category lasts \n",
        "\n",
        "        # assert False\n",
        "        if self.offset > 0:\n",
        "            self.sizeSamplers = [max(0, x - 1) for x in self.sizeSamplers]\n",
        "        # print(\"Size samplers:\\n\", self.sizeSamplers)\n",
        "        # print(\"Size samplers over batch size:\\n\", np.array(self.sizeSamplers) // self.batchSize)\n",
        "\n",
        "        order = [(x, torch.randperm(val).tolist())\n",
        "                 for x, val in enumerate(self.sizeSamplers) if\n",
        "                 val > 0]  # (index of seq/cat, randomly permuted numbers from 0 to num windows in seq(cat))\n",
        "\n",
        "        # Build Batches\n",
        "        self.batches = []\n",
        "        for indexSampler, randperm in order:\n",
        "            indexStart, sizeSampler = 0, self.sizeSamplers[indexSampler]\n",
        "            while indexStart < (sizeSampler - self.batchSize):\n",
        "                indexEnd = indexStart + self.batchSize\n",
        "                locBatch = [self.getIndex(x, indexSampler)\n",
        "                            for x in randperm[indexStart:indexEnd]]\n",
        "                indexStart = indexEnd\n",
        "                self.batches.append(locBatch)\n",
        "        # print(\"Number of batches:\\n\", len(self.batches))\n",
        "        # print(\"Batches:\\n\", self.batches)\n",
        "        # print(\"Batches shape: \\n\", np.array(self.batches).shape)\n",
        "        # print(\"Batches vstack shape: \\n\", np.vstack(self.batches).shape)\n",
        "        self.batches = np.vstack(self.batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def getIndex(self, x, iInterval):\n",
        "        return self.offset + x * self.sizeWindow + self.samplingIntervals[iInterval]\n",
        "\n",
        "    def __iter__(self):\n",
        "        random.shuffle(self.batches)\n",
        "        return iter(self.batches)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMMWgc3M4xln"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbjSx6SH4yo1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ChannelNorm(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 numFeatures,\n",
        "                 epsilon=1e-05,\n",
        "                 affine=True):\n",
        "\n",
        "        super(ChannelNorm, self).__init__()\n",
        "        if affine:\n",
        "            self.weight = nn.parameter.Parameter(torch.Tensor(1,\n",
        "                                                              numFeatures, 1))\n",
        "            self.bias = nn.parameter.Parameter(torch.Tensor(1, numFeatures, 1))\n",
        "        else:\n",
        "            self.weight = None\n",
        "            self.bias = None\n",
        "        self.epsilon = epsilon\n",
        "        self.p = 0\n",
        "        self.affine = affine\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.affine:\n",
        "            torch.nn.init.ones_(self.weight)\n",
        "            torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        cumMean = x.mean(dim=1, keepdim=True)\n",
        "        cumVar = x.var(dim=1, keepdim=True)\n",
        "        x = (x - cumMean) * torch.rsqrt(cumVar + self.epsilon)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            x = x * self.weight + self.bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class CPCEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 sizeHidden=512,\n",
        "                 normMode=\"layerNorm\"):\n",
        "\n",
        "        super(CPCEncoder, self).__init__()\n",
        "\n",
        "        validModes = [\"batchNorm\", \"instanceNorm\", \"ID\", \"layerNorm\"]\n",
        "        if normMode not in validModes:\n",
        "            raise ValueError(f\"Norm mode must be in {validModes}\")\n",
        "\n",
        "        if normMode == \"instanceNorm\":\n",
        "            def normLayer(x):\n",
        "                return nn.InstanceNorm1d(x, affine=True)\n",
        "        elif normMode == \"layerNorm\":\n",
        "            normLayer = ChannelNorm\n",
        "        else:\n",
        "            normLayer = nn.BatchNorm1d\n",
        "\n",
        "        self.dimEncoded = sizeHidden\n",
        "        self.conv0 = nn.Conv1d(1, sizeHidden, 10, stride=5, padding=3)\n",
        "        self.batchNorm0 = normLayer(sizeHidden)\n",
        "        self.conv1 = nn.Conv1d(sizeHidden, sizeHidden, 8, stride=4, padding=2)\n",
        "        self.batchNorm1 = normLayer(sizeHidden)\n",
        "        self.conv2 = nn.Conv1d(sizeHidden, sizeHidden, 4,\n",
        "                               stride=2, padding=1)\n",
        "        self.batchNorm2 = normLayer(sizeHidden)\n",
        "        self.conv3 = nn.Conv1d(sizeHidden, sizeHidden, 4, stride=2, padding=1)\n",
        "        self.batchNorm3 = normLayer(sizeHidden)\n",
        "        self.conv4 = nn.Conv1d(sizeHidden, sizeHidden, 4, stride=2, padding=1)\n",
        "        self.batchNorm4 = normLayer(sizeHidden)\n",
        "        self.DOWNSAMPLING = 160\n",
        "\n",
        "    def getDimOutput(self):\n",
        "        return self.conv4.out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.batchNorm0(self.conv0(x)))\n",
        "        x = torch.relu(self.batchNorm1(self.conv1(x)))\n",
        "        x = torch.relu(self.batchNorm2(self.conv2(x)))\n",
        "        x = torch.relu(self.batchNorm3(self.conv3(x)))\n",
        "        x = torch.relu(self.batchNorm4(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CPCAR(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dimEncoded,\n",
        "                 dimOutput,\n",
        "                 keepHidden,\n",
        "                 nLevelsGRU,\n",
        "                 mode=\"GRU\",\n",
        "                 reverse=False):\n",
        "\n",
        "        super(CPCAR, self).__init__()\n",
        "        self.RESIDUAL_STD = 0.1\n",
        "\n",
        "        if mode == \"LSTM\":\n",
        "            self.baseNet = nn.LSTM(dimEncoded, dimOutput,\n",
        "                                   num_layers=nLevelsGRU, batch_first=True)\n",
        "        elif mode == \"RNN\":\n",
        "            self.baseNet = nn.RNN(dimEncoded, dimOutput,\n",
        "                                  num_layers=nLevelsGRU, batch_first=True)\n",
        "        else:\n",
        "            self.baseNet = nn.GRU(dimEncoded, dimOutput,\n",
        "                                  num_layers=nLevelsGRU, batch_first=True)\n",
        "\n",
        "        self.hidden = None\n",
        "        self.keepHidden = keepHidden\n",
        "        self.reverse = reverse\n",
        "\n",
        "    def getDimOutput(self):\n",
        "        return self.baseNet.hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.reverse:\n",
        "            x = torch.flip(x, [1])\n",
        "        try:\n",
        "            self.baseNet.flatten_parameters()\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "        x, h = self.baseNet(x, self.hidden)\n",
        "        if self.keepHidden:\n",
        "            if isinstance(h, tuple):\n",
        "                self.hidden = tuple(x.detach() for x in h)\n",
        "            else:\n",
        "                self.hidden = h.detach()\n",
        "\n",
        "        # For better modularity, a sequence's order should be preserved\n",
        "        # by each module\n",
        "        if self.reverse:\n",
        "            x = torch.flip(x, [1])\n",
        "        return x\n",
        "\n",
        "\n",
        "class CPCModel(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 AR):\n",
        "        super(CPCModel, self).__init__()\n",
        "        self.gEncoder = encoder\n",
        "        self.gAR = AR\n",
        "\n",
        "    def forward(self, batchData, label):\n",
        "        encodedData = self.gEncoder(batchData).permute(0, 2, 1)\n",
        "        cFeature = self.gAR(encodedData)\n",
        "        return cFeature, encodedData, label\n",
        "\n",
        "\n",
        "class PredictionNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nPredicts,\n",
        "                 dimOutputAR,\n",
        "                 dimOutputEncoder,\n",
        "                 dropout=False):\n",
        "\n",
        "        super(PredictionNetwork, self).__init__()\n",
        "        self.predictors = nn.ModuleList()\n",
        "        self.RESIDUAL_STD = 0.01\n",
        "        self.dimOutputAR = dimOutputAR\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5) if dropout else None\n",
        "        for i in range(nPredicts):\n",
        "            self.predictors.append(\n",
        "                nn.Linear(dimOutputAR, dimOutputEncoder, bias=False))\n",
        "            if dimOutputEncoder > dimOutputAR:\n",
        "                residual = dimOutputEncoder - dimOutputAR\n",
        "                self.predictors[-1].weight.data.copy_(torch.cat([torch.randn(\n",
        "                    dimOutputAR, dimOutputAR), self.RESIDUAL_STD * torch.randn(residual, dimOutputAR)], dim=0))\n",
        "\n",
        "    def forward(self, c, candidates):\n",
        "\n",
        "        assert (len(candidates) == len(self.predictors))\n",
        "        out = []\n",
        "\n",
        "        # UGLY\n",
        "        # if isinstance(self.predictors[0], EqualizedConv1d):\n",
        "        # c = c.permute(0, 2, 1)\n",
        "\n",
        "        for k in range(len(self.predictors)):\n",
        "\n",
        "            locC = self.predictors[k](c)\n",
        "            if isinstance(locC, tuple):\n",
        "                locC = locC[0]\n",
        "            # if isinstance(self.predictors[k], EqualizedConv1d):\n",
        "            # locC = locC.permute(0, 2, 1)\n",
        "            if self.dropout is not None:\n",
        "                locC = self.dropout(locC)\n",
        "            locC = locC.view(locC.size(0), 1, locC.size(1), locC.size(2))\n",
        "            outK = (locC * candidates[k]).mean(dim=3)\n",
        "            out.append(outK)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BaseCriterion(nn.Module):\n",
        "    def update(self):\n",
        "        return\n",
        "\n",
        "\n",
        "class CPCUnsupersivedCriterion(BaseCriterion):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nPredicts,  # Number of steps\n",
        "                 dimOutputAR,  # Dimension of G_ar\n",
        "                 dimOutputEncoder,  # Dimension of the convolutional net\n",
        "                 negativeSamplingExt,  # Number of negative samples to draw\n",
        "                 mode=None,\n",
        "                 dropout=False):\n",
        "\n",
        "        super(CPCUnsupersivedCriterion, self).__init__()\n",
        "\n",
        "        self.wPrediction = PredictionNetwork(\n",
        "            nPredicts, dimOutputAR, dimOutputEncoder, dropout=dropout)\n",
        "        self.nPredicts = nPredicts\n",
        "        self.negativeSamplingExt = negativeSamplingExt\n",
        "        self.lossCriterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        if mode not in [None, \"reverse\"]:\n",
        "            raise ValueError(\"Invalid mode\")\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "    def sampleClean(self, encodedData, windowSize):\n",
        "\n",
        "        batchSize, nNegativeExt, dimEncoded = encodedData.size()\n",
        "        outputs = []\n",
        "\n",
        "        negExt = encodedData.contiguous().view(-1, dimEncoded)\n",
        "        # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
        "        batchIdx = torch.randint(low=0, high=batchSize,\n",
        "                                 size=(self.negativeSamplingExt\n",
        "                                       * windowSize * batchSize,),\n",
        "                                 device=encodedData.device)\n",
        "\n",
        "        seqIdx = torch.randint(low=1, high=nNegativeExt,\n",
        "                               size=(self.negativeSamplingExt\n",
        "                                     * windowSize * batchSize,),\n",
        "                               device=encodedData.device)\n",
        "\n",
        "        baseIdx = torch.arange(0, windowSize, device=encodedData.device)\n",
        "        baseIdx = baseIdx.view(1, 1,\n",
        "                               windowSize).expand(1,\n",
        "                                                  self.negativeSamplingExt,\n",
        "                                                  windowSize).expand(batchSize, self.negativeSamplingExt, windowSize)\n",
        "        seqIdx += baseIdx.contiguous().view(-1)\n",
        "        seqIdx = torch.remainder(seqIdx, nNegativeExt)\n",
        "\n",
        "        extIdx = seqIdx + batchIdx * nNegativeExt\n",
        "        negExt = negExt[extIdx].view(batchSize, self.negativeSamplingExt,\n",
        "                                     windowSize, dimEncoded)\n",
        "\n",
        "        labelLoss = torch.zeros((batchSize * windowSize),\n",
        "                                dtype=torch.long,\n",
        "                                device=encodedData.device)\n",
        "\n",
        "        for k in range(1, self.nPredicts + 1):\n",
        "\n",
        "            # Positive samples\n",
        "            if k < self.nPredicts:\n",
        "                posSeq = encodedData[:, k:-(self.nPredicts - k)]\n",
        "            else:\n",
        "                posSeq = encodedData[:, k:]\n",
        "\n",
        "            posSeq = posSeq.view(batchSize, 1, windowSize, dimEncoded)\n",
        "            fullSeq = torch.cat((posSeq, negExt), dim=1)\n",
        "            outputs.append(fullSeq)\n",
        "\n",
        "        return outputs, labelLoss\n",
        "\n",
        "    def getInnerLoss(self):\n",
        "\n",
        "        return \"orthoLoss\", self.orthoLoss * self.wPrediction.orthoCriterion()\n",
        "\n",
        "    def forward(self, cFeature, encodedData):\n",
        "\n",
        "        if self.mode == \"reverse\":\n",
        "            encodedData = torch.flip(encodedData, [1])\n",
        "            cFeature = torch.flip(cFeature, [1])\n",
        "\n",
        "        batchSize, seqSize, dimAR = cFeature.size()\n",
        "        windowSize = seqSize - self.nPredicts\n",
        "\n",
        "        cFeature = cFeature[:, :windowSize]\n",
        "\n",
        "        sampledData, labelLoss = self.sampleClean(encodedData, windowSize)\n",
        "\n",
        "        predictions = self.wPrediction(cFeature, sampledData)\n",
        "\n",
        "        outLosses = [0 for _ in range(self.nPredicts)]\n",
        "        outAcc = [0 for _ in range(self.nPredicts)]\n",
        "\n",
        "        for k, locPreds in enumerate(predictions[:self.nPredicts]):\n",
        "            locPreds = locPreds.permute(0, 2, 1)  # (batchSize, 1 + negativeSamplingExt, windowSize) to\n",
        "            #                                       (batchSize, windowSize, 1 + negativeSamplingExt)\n",
        "            locPreds = locPreds.contiguous().view(\n",
        "                -1, locPreds.size(2))  # (batchSize, windowSize, 1 + negativeSamplingExt) to\n",
        "            #                            (batchSize * windowSize, 1 + negativeSamplingExt)\n",
        "            lossK = self.lossCriterion(locPreds, labelLoss)\n",
        "            outLosses[k] += lossK.view(1, -1)\n",
        "            _, predsIndex = locPreds.max(1)\n",
        "            outAcc[k] += torch.sum(predsIndex == labelLoss).float().view(1, -1)\n",
        "\n",
        "        return torch.cat(outLosses, dim=1), torch.cat(outAcc, dim=1) / (windowSize * batchSize)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJSRs0F43UA"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtqKVS-444yK"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from copy import deepcopy\n",
        "# import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "\n",
        "def update_logs(logs, logStep, prevlogs=None):\n",
        "    out = {}\n",
        "    for key in logs:\n",
        "        out[key] = deepcopy(logs[key])\n",
        "\n",
        "        if prevlogs is not None:\n",
        "            out[key] -= prevlogs[key]\n",
        "        out[key] /= logStep\n",
        "    return out\n",
        "\n",
        "\n",
        "def save_logs(data, pathLogs):\n",
        "    with open(pathLogs, 'w') as file:\n",
        "        json.dump(data, file, indent=2)\n",
        "\n",
        "\n",
        "def save_checkpoint(model_state, criterion_state, optimizer_state, best_state,\n",
        "                    path_checkpoint):\n",
        "\n",
        "    state_dict = {\"gEncoder\": model_state,\n",
        "                  \"cpcCriterion\": criterion_state,\n",
        "                  \"optimizer\": optimizer_state,\n",
        "                  \"best\": best_state}\n",
        "\n",
        "    torch.save(state_dict, path_checkpoint)\n",
        "\n",
        "\n",
        "def show_logs(text, logs):\n",
        "    print(\"\")\n",
        "    print('-' * 50)\n",
        "    print(text)\n",
        "\n",
        "    for key in logs:\n",
        "\n",
        "        if key == \"iter\":\n",
        "            continue\n",
        "\n",
        "        nPredicts = logs[key].shape[0]\n",
        "\n",
        "        strSteps = ['Step'] + [str(s) for s in range(1, nPredicts + 1)]\n",
        "        formatCommand = ' '.join(['{:>16}' for _ in range(nPredicts + 1)])\n",
        "        print(formatCommand.format(*strSteps))\n",
        "\n",
        "        strLog = [key] + [\"{:10.6f}\".format(s) for s in logs[key]]\n",
        "        print(formatCommand.format(*strLog))\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "def trainStep(dataLoader,\n",
        "              cpcModel,\n",
        "              cpcCriterion,\n",
        "              optimizer,\n",
        "              loggingStep,\n",
        "              useGPU,\n",
        "              log2Board=0,\n",
        "              totalSteps=0):\n",
        "    cpcModel.train()\n",
        "    cpcCriterion.train()\n",
        "\n",
        "    startTime = time.perf_counter()\n",
        "    n_examples = 0\n",
        "    logs, lastlogs = {}, None\n",
        "    iterCtr = 0\n",
        "\n",
        "    if log2Board > 2:\n",
        "        gradmapGEncoder = {}\n",
        "        gradmapGAR = {}\n",
        "        gradmapWPrediction = {}\n",
        "\n",
        "    for step, fulldata in enumerate(dataLoader):\n",
        "        batchData, label = fulldata\n",
        "        n_examples += batchData.size(0)\n",
        "        if useGPU:\n",
        "            batchData = batchData.cuda(non_blocking=True)\n",
        "            label = label.cuda(non_blocking=True)\n",
        "        c_feature, encoded_data, label = cpcModel(batchData, label)\n",
        "        allLosses, allAcc = cpcCriterion(c_feature, encoded_data)\n",
        "        totLoss = allLosses.sum()\n",
        "\n",
        "        totLoss.backward()\n",
        "\n",
        "        if log2Board > 2:\n",
        "            gradmapGEncoder = updateGradientMap(cpcModel.gEncoder, gradmapGEncoder)\n",
        "            gradmapGAR = updateGradientMap(cpcModel.gAR, gradmapGAR)\n",
        "            gradmapWPrediction = updateGradientMap(cpcCriterion.wPrediction, gradmapWPrediction)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if \"locLoss_train\" not in logs:\n",
        "            logs[\"locLoss_train\"] = np.zeros(allLosses.size(1))\n",
        "            logs[\"locAcc_train\"] = np.zeros(allLosses.size(1))\n",
        "\n",
        "        logs[\"locLoss_train\"] += (allLosses.mean(dim=0)).detach().cpu().numpy()\n",
        "        logs[\"locAcc_train\"] += (allAcc.mean(dim=0)).cpu().numpy()\n",
        "\n",
        "        if log2Board > 1:\n",
        "            for t in range(len(logs[\"locLoss_train\"])):\n",
        "                experiment.log_metric(f\"Losses/batch/locLoss_train_{t}\", logs[\"locLoss_train\"][t], step=totalSteps + iterCtr)\n",
        "                experiment.log_metric(f\"Accuracy/batch/locAcc_train_{t}\", logs[\"locAcc_train\"][t], step=totalSteps + iterCtr)\n",
        "        iterCtr += 1\n",
        "\n",
        "        if (step + 1) % loggingStep == 0:\n",
        "            new_time = time.perf_counter()\n",
        "            elapsed = new_time - startTime\n",
        "            print(f\"Update {step + 1}\")\n",
        "            print(f\"elapsed: {elapsed:.1f} s\")\n",
        "            print(\n",
        "                f\"{1000.0 * elapsed / loggingStep:.1f} ms per batch, {1000.0 * elapsed / n_examples:.1f} ms / example\")\n",
        "            locLogs = update_logs(logs, loggingStep, lastlogs)\n",
        "            lastlogs = deepcopy(logs)\n",
        "            show_logs(\"Training loss\", locLogs)\n",
        "            startTime, n_examples = new_time, 0\n",
        "\n",
        "    if log2Board > 2:\n",
        "        # Scale and log gradients\n",
        "        for k, v in gradmapGEncoder.items():\n",
        "            gradmapGEncoder[k] = v / iterCtr\n",
        "        for k, v in gradmapGAR.items():\n",
        "            gradmapGAR[k] = v / iterCtr\n",
        "        for k, v in gradmapWPrediction.items():\n",
        "            gradmapWPrediction[k] = v / iterCtr\n",
        "        logGradients(gradmapGEncoder, totalSteps)\n",
        "        logGradients(gradmapGAR, totalSteps)\n",
        "        logGradients(gradmapWPrediction, totalSteps)\n",
        "\n",
        "    logs = update_logs(logs, iterCtr)\n",
        "    logs[\"iter\"] = iterCtr\n",
        "    show_logs(\"Average training loss on epoch\", logs)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def valStep(dataLoader,\n",
        "            cpcModel,\n",
        "            cpcCriterion,\n",
        "            useGPU):\n",
        "    cpcCriterion.eval()\n",
        "    cpcModel.eval()\n",
        "    logs = {}\n",
        "    cpcCriterion.eval()\n",
        "    cpcModel.eval()\n",
        "    iterCtr = 0\n",
        "\n",
        "    for step, fulldata in enumerate(dataLoader):\n",
        "\n",
        "        batchData, label = fulldata\n",
        "\n",
        "        if useGPU:\n",
        "            batchData = batchData.cuda(non_blocking=True)\n",
        "            label = label.cuda(non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            c_feature, encoded_data, label = cpcModel(batchData, label)\n",
        "            allLosses, allAcc = cpcCriterion(c_feature, encoded_data)\n",
        "\n",
        "        if \"locLoss_val\" not in logs:\n",
        "            logs[\"locLoss_val\"] = np.zeros(allLosses.size(1))\n",
        "            logs[\"locAcc_val\"] = np.zeros(allLosses.size(1))\n",
        "\n",
        "        iterCtr += 1\n",
        "        logs[\"locLoss_val\"] += allLosses.mean(dim=0).cpu().numpy()\n",
        "        logs[\"locAcc_val\"] += allAcc.mean(dim=0).cpu().numpy()\n",
        "\n",
        "    logs = update_logs(logs, iterCtr)\n",
        "    logs[\"iter\"] = iterCtr\n",
        "    show_logs(\"Validation loss:\", logs)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def updateGradientMap(model, gradMap):\n",
        "    for name, layer in zip(model._modules, model.children()):\n",
        "        if \"activ\" in name:\n",
        "            continue\n",
        "        if not hasattr(layer, \"weight\"):\n",
        "            continue\n",
        "        weightName = \"%s/%s.%s\" % (\"Gradients\", name, \"weight\")\n",
        "        biasName = \"%s/%s.%s\" % (\"Gradients\", name, \"bias\")\n",
        "        gradMap.setdefault(weightName, 0)\n",
        "        gradMap.setdefault(biasName, 0)\n",
        "        gradMap[weightName] += layer.weight.grad\n",
        "        gradMap[biasName] += layer.bias.grad\n",
        "    return gradMap\n",
        "\n",
        "\n",
        "def logGradients(gradMap, step):\n",
        "    for k, v in gradMap.items():\n",
        "        experiment.log_histogram_3d(v.detach().numpy(), name=k, step=step)\n",
        "\n",
        "def logWeights(model, step):\n",
        "    for name, layer in zip(model._modules, model.children()):\n",
        "        if \"activ\" in name:\n",
        "            continue\n",
        "        if not hasattr(layer, \"weight\"):\n",
        "            continue\n",
        "        weightName = \"%s/%s.%s\" % (\"Parameters\", name, \"weight\")\n",
        "        biasName = \"%s/%s.%s\" % (\"Parameters\", name, \"bias\")\n",
        "        experiment.log_histogram_3d(layer.weight.detach().numpy(), name=weightName, step=step)\n",
        "        experiment.log_histogram_3d(layer.bias.detach().numpy(), name=biasName, step=step)\n",
        "\n",
        "def trainingLoop(trainDataset,\n",
        "                 valDataset,\n",
        "                 batchSize,\n",
        "                 samplingMode,\n",
        "                 cpcModel,\n",
        "                 cpcCriterion,\n",
        "                 nEpoch,\n",
        "                 optimizer,\n",
        "                 pathCheckpoint,\n",
        "                 logs,\n",
        "                 useGPU,\n",
        "                 log2Board=0):\n",
        "    print(f\"Running {nEpoch} epochs\")\n",
        "    startEpoch = len(logs[\"epoch\"])\n",
        "    bestAcc = 0\n",
        "    bestStateDict = None\n",
        "    startTime = time.time()\n",
        "    epoch = 0\n",
        "    totalSteps = 0\n",
        "    try:\n",
        "        for epoch in range(startEpoch, nEpoch):\n",
        "            \n",
        "            if log2Board > 2:\n",
        "                logWeights(cpcModel.gEncoder, epoch)\n",
        "                logWeights(cpcModel.gAR, epoch)\n",
        "                logWeights(cpcCriterion.wPrediction, epoch)\n",
        "\n",
        "            print(f\"Starting epoch {epoch}\")\n",
        "            trainLoader = trainDataset.getDataLoader(batchSize, samplingMode,\n",
        "                                                    True, numWorkers=0)\n",
        "            valLoader = valDataset.getDataLoader(batchSize, 'sequential', False,\n",
        "                                                numWorkers=0)\n",
        "\n",
        "            print(\"Training dataset %d batches, Validation dataset %d batches, batch size %d\" %\n",
        "                (len(trainLoader), len(valLoader), batchSize))\n",
        "\n",
        "            locLogsTrain = trainStep(trainLoader, cpcModel, cpcCriterion, optimizer, logs[\"logging_step\"], \n",
        "                                        useGPU, log2Board, totalSteps)\n",
        "\n",
        "            totalSteps += locLogsTrain['iter']\n",
        "\n",
        "            locLogsVal = valStep(valLoader, cpcModel, cpcCriterion, useGPU)\n",
        "\n",
        "            print(f'Ran {epoch + 1} epochs '\n",
        "                f'in {time.time() - startTime:.2f} seconds')\n",
        "\n",
        "            if useGPU:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            currentAccuracy = float(locLogsVal[\"locAcc_val\"].mean())\n",
        "            \n",
        "            if log2Board:\n",
        "                for t in range(len(locLogsVal[\"locLoss_val\"])):\n",
        "                    experiment.log_metric(f\"Losses/epoch/locLoss_train_{t}\", locLogsTrain[\"locLoss_train\"][t], step=epoch)\n",
        "                    experiment.log_metric(f\"Accuracy/epoch/locAcc_train_{t}\", locLogsTrain[\"locAcc_train\"][t], step=epoch)\n",
        "                    experiment.log_metric(f\"Losses/epoch/locLoss_val_{t}\", locLogsVal[\"locLoss_val\"][t], step=epoch)\n",
        "                    experiment.log_metric(f\"Accuracy/epoch/locAcc_val_{t}\", locLogsVal[\"locAcc_val\"][t], step=epoch)\n",
        "\n",
        "            if currentAccuracy > bestAcc:\n",
        "                bestStateDict = cpcModel.state_dict()\n",
        "\n",
        "            for key, value in dict(locLogsTrain, **locLogsVal).items():\n",
        "                if key not in logs:\n",
        "                    logs[key] = [None for _ in range(epoch)]\n",
        "                if isinstance(value, np.ndarray):\n",
        "                    value = value.tolist()\n",
        "                logs[key].append(value)\n",
        "\n",
        "            logs[\"epoch\"].append(epoch)\n",
        "\n",
        "            if pathCheckpoint is not None and (epoch % logs[\"saveStep\"] == 0 or epoch == nEpoch - 1):\n",
        "                modelStateDict = cpcModel.state_dict()\n",
        "                criterionStateDict = cpcCriterion.state_dict()\n",
        "\n",
        "                save_checkpoint(modelStateDict, criterionStateDict, optimizer.state_dict(), bestStateDict,\n",
        "                                f\"{pathCheckpoint}_{epoch}.pt\")\n",
        "                save_logs(logs, pathCheckpoint + \"_logs.json\")\n",
        "    except KeyboardInterrupt:\n",
        "        if pathCheckpoint is not None:\n",
        "            modelStateDict = cpcModel.state_dict()\n",
        "            criterionStateDict = cpcCriterion.state_dict()\n",
        "\n",
        "            save_checkpoint(modelStateDict, criterionStateDict, optimizer.state_dict(), bestStateDict,\n",
        "                            f\"{pathCheckpoint}_{epoch}_interrupted.pt\")\n",
        "            save_logs(logs, pathCheckpoint + \"_logs.json\")\n",
        "        return\n",
        "\n",
        "def run(trainDataset,\n",
        "        valDataset,\n",
        "        batchSize,\n",
        "        samplingMode,\n",
        "        cpcModel,\n",
        "        cpcCriterion,\n",
        "        nEpoch,\n",
        "        optimizer,\n",
        "        pathCheckpoint,\n",
        "        logs,\n",
        "        useGPU,\n",
        "        log2Board=0):\n",
        "    if log2Board:\n",
        "        with experiment.train():\n",
        "            trainingLoop(trainDataset, valDataset, batchSize, samplingMode, cpcModel, cpcCriterion, nEpoch, optimizer,\n",
        "                         pathCheckpoint, logs, useGPU, log2Board)\n",
        "            experiment.end()\n",
        "    else:\n",
        "        trainingLoop(trainDataset, valDataset, batchSize, samplingMode, cpcModel, cpcCriterion, nEpoch, optimizer, \n",
        "                     pathCheckpoint, logs, useGPU, log2Board)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1undhM04n55"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpthiUBiVZ_W"
      },
      "source": [
        "import torch\n",
        "# from dataloader import AudioBatchData\n",
        "# from model import CPCEncoder, CPCAR, CPCModel, CPCUnsupersivedCriterion\n",
        "# from trainer import run\n",
        "from datetime import datetime\n",
        "import os"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f1IN1MRVdDy",
        "outputId": "29fbf2ce-640f-4177-dccc-0067fd795d43"
      },
      "source": [
        "labelsBy = 'ensemble'\n",
        "print(\"Loading the training dataset\")\n",
        "trainDataset = AudioBatchData(rawAudioPath='data/musicnet_lousy/train_data',\n",
        "                                metadataPath='data/musicnet_lousy/metadata_train.csv',\n",
        "                                sizeWindow=20480,\n",
        "                                labelsBy=labelsBy,\n",
        "                                outputPath='data/musicnet_lousy/train_data/train',\n",
        "                                CHUNK_SIZE=1e9,\n",
        "                                NUM_CHUNKS_INMEM=1)\n",
        "print(\"Training dataset loaded\")\n",
        "print(\"\")\n",
        "\n",
        "print(\"Loading the validation dataset\")\n",
        "valDataset = AudioBatchData(rawAudioPath='data/musicnet_lousy/train_data',\n",
        "                            metadataPath='data/musicnet_lousy/metadata_val.csv',\n",
        "                            sizeWindow=20480,\n",
        "                            labelsBy=labelsBy,\n",
        "                            outputPath='data/musicnet_lousy/train_data/val',\n",
        "                            CHUNK_SIZE=1e9,\n",
        "                            NUM_CHUNKS_INMEM=1)\n",
        "print(\"Validation dataset loaded\")\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the training dataset\n",
            "Chunks already exist at data/musicnet_lousy/train_data/train/ensemble\n",
            "Loading files\n",
            "Loaded 37 sequences, elapsed=21.416 secs\n",
            "Training dataset loaded\n",
            "\n",
            "Loading the validation dataset\n",
            "Chunks already exist at data/musicnet_lousy/train_data/val/ensemble\n",
            "Loading files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBri49jRXtf0"
      },
      "source": [
        "samplingType = 'samesequence'\n",
        "\n",
        "# Encoder network\n",
        "encoderNet = CPCEncoder(512, 'layerNorm')\n",
        "# AR Network\n",
        "arNet = CPCAR(512, 256, samplingType == 'sequential', 1, mode=\"GRU\", reverse=False)\n",
        "\n",
        "cpcModel = CPCModel(encoderNet, arNet)\n",
        "batchSize = 8\n",
        "cpcModel.supervised = False\n",
        "\n",
        "cpcCriterion = CPCUnsupersivedCriterion(nPredicts=12,\n",
        "                                        dimOutputAR=256,\n",
        "                                        dimOutputEncoder=512,\n",
        "                                        negativeSamplingExt=128,\n",
        "                                        mode=None,\n",
        "                                        dropout=False)\n",
        "useGPU = torch.cuda.is_available()\n",
        "\n",
        "if useGPU:\n",
        "    cpcCriterion.cuda()\n",
        "    cpcModel.cuda()\n",
        "\n",
        "gParams = list(cpcCriterion.parameters()) + list(cpcModel.parameters())\n",
        "lr = 2e-4\n",
        "optimizer = torch.optim.Adam(gParams, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "\n",
        "expDescription = f'{samplingType}_'\n",
        "if samplingType == 'samecategory':\n",
        "    expDescription += f'{labelsBy}_'\n",
        "\n",
        "pathCheckpoint = f'logs/{expDescription}{datetime.now().strftime(\"%d-%m_%H-%M-%S\")}'\n",
        "if not os.path.isdir(pathCheckpoint):\n",
        "    os.mkdir(pathCheckpoint)\n",
        "pathCheckpoint = os.path.join(pathCheckpoint, \"checkpoint\")\n",
        "\n",
        "logs = {\"epoch\": [], \"iter\": [], \"saveStep\": 1, \"logging_step\": 1000}\n",
        "\n",
        "run(trainDataset, valDataset, batchSize, samplingType, cpcModel, cpcCriterion, 30, optimizer, pathCheckpoint, logs, \n",
        "    useGPU, log2Board=3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}