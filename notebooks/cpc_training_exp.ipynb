{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cpc_training_exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS3zre3hVRaa",
        "outputId": "5a6feea0-99e6-4918-b7ec-a7aec0f99dc4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/DLProjects/JTM\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/DLProjects/JTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmv_AG1ws6Sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdcb71e-f44b-432f-e0a3-f5a948ebe4d0"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4W0oTBeW8q_",
        "outputId": "03f701a1-1788-41b9-9c88-9771869956e1"
      },
      "source": [
        "!pip install comet_ml"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting comet_ml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/5a/50260282d44740a757d17829e9c6c4a7a90430f2c484bacbdcc31c6a4ab3/comet_ml-3.10.0-py2.py3-none-any.whl (258kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 61kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 133kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 143kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 153kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 163kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 174kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 184kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 194kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 204kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 215kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 225kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 235kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 245kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 256kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 6.8MB/s \n",
            "\u001b[?25hCollecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ac/b7082c3d228e600af37ec5cf99697d400328b13350b4d7577c213fa4faca/wurlitzer-2.1.0-py2.py3-none-any.whl\n",
            "Collecting requests-toolbelt>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.15.0)\n",
            "Collecting dulwich>=0.20.6; python_version >= \"3.0\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/f0/dab35f0491fa36cfc28abd48f150885a08a8726900da25549dbd461bf115/dulwich-0.20.23-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (529kB)\n",
            "\u001b[K     |████████████████████████████████| 532kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.12.1)\n",
            "Collecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/ee/7aa724dc2dbed9b028f463eada5482770c13b7381a0c79457d12b3b62de2/websocket_client-1.0.1-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.1MB/s \n",
            "\u001b[?25hCollecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/dc/38593280ec30fe1cb2611ec65554b76b68d13582bf490113e3332cdd85ea/everett-1.0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Collecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp37-none-any.whl size=34547 sha256=e53f8febc4e2c432bed10c3f76d638eb412dd48229fd041b678e8d9f86c74e51\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "Installing collected packages: wurlitzer, requests-toolbelt, dulwich, websocket-client, configobj, everett, comet-ml\n",
            "Successfully installed comet-ml-3.10.0 configobj-5.0.6 dulwich-0.20.23 everett-1.0.3 requests-toolbelt-0.9.1 websocket-client-1.0.1 wurlitzer-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq4_HbGQ8UBL",
        "outputId": "b1cfcf20-2895-445c-c404-91f731e78705"
      },
      "source": [
        "from comet_ml import Experiment\n",
        "import comet_ml\n",
        "\n",
        "comet_ml.init(project_name=\"jtm\", workspace=\"tiagocuervo\")\n",
        "experiment = Experiment()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter your Comet API key from https://www.comet.ml/api/my/settings\n",
            "(api key may not show as you type)\n",
            "Comet API key: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Comet API key is valid\n",
            "COMET INFO: Comet API key saved in /root/.comet.config\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/tiagocuervo/jtm/12ce6792e0ac4a0ab58ad284ec940ce9\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0pN8vB4qV6"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w4ZEQ4r4s5p"
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import Sampler, BatchSampler\n",
        "import librosa\n",
        "import tqdm\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AudioBatchData(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 rawAudioPath,\n",
        "                 metadataPath,\n",
        "                 sizeWindow,\n",
        "                 labelsBy='composer',\n",
        "                 outputPath=None,\n",
        "                 CHUNK_SIZE=1e9,\n",
        "                 NUM_CHUNKS_INMEM=2,\n",
        "                 useGPU=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            - rawAudioPath (string): path to the raw audio files\n",
        "            - metadataPath (string): path to the data set metadata (used to define labels)\n",
        "            - sizeWindow (int): size of the sliding window\n",
        "            - labelsBy (string): name of column in metadata according to which create labels\n",
        "            - outputPath (string): path to the directory where chunks are to be created or are stored\n",
        "            - CHUNK_SIZE (int): desired size in bytes of a chunk\n",
        "            - NUM_CHUNKS_INMEM (int): target maximal size chunks of data to load in memory at a time\n",
        "        \"\"\"\n",
        "        self.NUM_CHUNKS_INMEM = NUM_CHUNKS_INMEM\n",
        "        self.CHUNK_SIZE = CHUNK_SIZE\n",
        "        self.rawAudioPath = Path(rawAudioPath)\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.useGPU = useGPU\n",
        "\n",
        "        self.sequencesData = pd.read_csv(metadataPath, index_col='id')\n",
        "        self.sequencesData = self.sequencesData.sort_values(by=labelsBy)\n",
        "        self.sequencesData[labelsBy] = self.sequencesData[labelsBy].astype('category')\n",
        "        self.sequencesData[labelsBy] = self.sequencesData[labelsBy].cat.codes\n",
        "\n",
        "        self.totSize = self.sequencesData['length'].sum()\n",
        "        # print(\"Total size:\", self.totSize)\n",
        "        # print(\"Length of data set:\", self.__len__())\n",
        "\n",
        "        self.category = labelsBy\n",
        "\n",
        "        if outputPath is None:\n",
        "            self.chunksDir = self.rawAudioPath / labelsBy\n",
        "        else:\n",
        "            self.chunksDir = Path(outputPath) / labelsBy\n",
        "\n",
        "        if not os.path.exists(self.chunksDir):\n",
        "            os.makedirs(self.chunksDir)\n",
        "\n",
        "        packages2Load = [fileName for fileName in os.listdir(self.chunksDir) if\n",
        "                         re.match(r'chunk_.*[0-9]+.pickle', fileName)]\n",
        "\n",
        "        if len(packages2Load) == 0:\n",
        "            self._createChunks()\n",
        "            packages2Load = [fileName for fileName in os.listdir(self.chunksDir) if\n",
        "                             re.match(r'chunk_.*[0-9]+.pickle', fileName)]\n",
        "        else:\n",
        "            print(\"Chunks already exist at\", self.chunksDir)\n",
        "\n",
        "        self.packs = []\n",
        "        packOfChunks = []\n",
        "        for i, packagePath in enumerate(packages2Load):\n",
        "            packOfChunks.append(packagePath)\n",
        "            if (i + 1) % self.NUM_CHUNKS_INMEM == 0:\n",
        "                self.packs.append(packOfChunks)\n",
        "                packOfChunks = []\n",
        "        if len(packOfChunks) > 0:\n",
        "            self.packs.append(packOfChunks)\n",
        "\n",
        "        self.currentPack = -1\n",
        "        self.nextPack = 0\n",
        "        self.sequenceIdx = 0\n",
        "\n",
        "        self.data = None\n",
        "\n",
        "        self._loadNextPack(first=True)\n",
        "        self._loadNextPack()\n",
        "\n",
        "    def _createChunks(self):\n",
        "        print(\"Creating chunks at\", self.chunksDir)\n",
        "        pack = []\n",
        "        packIds = []\n",
        "        packageSize = 0\n",
        "        packageIdx = 0\n",
        "        for trackId in tqdm.tqdm(self.sequencesData.index):\n",
        "            sequence, samplingRate = librosa.load(self.rawAudioPath / (str(trackId) + '.wav'), sr=16000)\n",
        "            sequence = torch.tensor(sequence).float()\n",
        "            packIds.append(trackId)\n",
        "            pack.append(sequence)\n",
        "            packageSize += len(sequence) * 4\n",
        "            if packageSize >= self.CHUNK_SIZE:\n",
        "                print(f\"Saved pack {packageIdx}\")\n",
        "                with open(self.chunksDir / f'chunk_{packageIdx}.pickle', 'wb') as handle:\n",
        "                    pickle.dump(torch.cat(pack, dim=0), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                with open(self.chunksDir / f'ids_{packageIdx}.pickle', 'wb') as handle:\n",
        "                    pickle.dump(packIds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                pack = []\n",
        "                packIds = []\n",
        "                packageSize = 0\n",
        "                packageIdx += 1\n",
        "        print(f\"Saved pack {packageIdx}\")\n",
        "        with open(self.chunksDir / f'chunk_{packageIdx}.pickle', 'wb') as handle:\n",
        "            pickle.dump(torch.cat(pack, dim=0), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.chunksDir / f'ids_{packageIdx}.pickle', 'wb') as handle:\n",
        "            pickle.dump(packIds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def _loadNextPack(self, first=False):\n",
        "        self.clear()\n",
        "        if not first:\n",
        "            self.currentPack = self.nextPack\n",
        "            startTime = time.time()\n",
        "            print('Loading files')\n",
        "            self.categoryLabel = [0]\n",
        "            packageIdx = [0]\n",
        "            self.seqLabel = [0]\n",
        "            packageSize = 0\n",
        "            with open(self.chunksDir / (\n",
        "                'ids_' + self.packs[self.currentPack][0].split('_', maxsplit=1)[-1]), 'rb') as handle:\n",
        "                    chunkIds = pickle.load(handle)\n",
        "            previousCategory = self.sequencesData.loc[chunkIds[0]][self.category]\n",
        "            for packagePath in self.packs[self.currentPack]:\n",
        "                with open(self.chunksDir / ('ids_' + packagePath.split('_', maxsplit=1)[-1]), 'rb') as handle:\n",
        "                    chunkIds = pickle.load(handle)\n",
        "                for seqId in chunkIds:\n",
        "                    currentCategory = self.sequencesData.loc[seqId][self.category]\n",
        "                    if currentCategory != previousCategory:\n",
        "                        self.categoryLabel.append(packageSize)\n",
        "                    previousCategory = currentCategory\n",
        "                    packageSize += self.sequencesData.loc[seqId].length\n",
        "                    self.seqLabel.append(packageSize)\n",
        "                self.categoryLabel.append(packageSize)\n",
        "                packageIdx.append(packageSize)\n",
        "\n",
        "            self.data = torch.empty(size=(packageSize,))\n",
        "            \n",
        "            for i, packagePath in enumerate(self.packs[self.currentPack]):\n",
        "                with open(self.chunksDir / packagePath, 'rb') as handle:\n",
        "                    self.data[packageIdx[i]:packageIdx[i + 1]] = pickle.load(handle)\n",
        "            if self.useGPU:\n",
        "                self.data = self.data.cuda(non_blocking=True)\n",
        "                print(\"Data moved to GPU\")\n",
        "                print(\"Data in: \", self.data.device)\n",
        "            print(f'Loaded {len(self.seqLabel) - 1} sequences, elapsed={time.time() - startTime:.3f} secs')\n",
        "\n",
        "        self.nextPack = (self.currentPack + 1) % len(self.packs)\n",
        "        if self.nextPack == 0 and len(self.packs) > 1:\n",
        "            self.currentPack = -1\n",
        "            self.nextPack = 0\n",
        "            self.sequenceIdx = 0\n",
        "\n",
        "    def clear(self):\n",
        "        if 'data' in self.__dict__:\n",
        "            del self.data\n",
        "        if 'categoryLabel' in self.__dict__:\n",
        "            del self.categoryLabel\n",
        "        if 'seqLabel' in self.__dict__:\n",
        "            del self.seqLabel\n",
        "\n",
        "    def getCategoryLabel(self, idx):\n",
        "        idCategory = next(x[0] for x in enumerate(self.categoryLabel) if x[1] > idx) - 1\n",
        "        return idCategory\n",
        "\n",
        "    def getSequenceLabel(self, idx):\n",
        "        return self.categoryLabel[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.totSize // self.sizeWindow\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < 0 or idx >= len(self.data) - self.sizeWindow - 1:\n",
        "            print(idx)\n",
        "\n",
        "        outData = self.data[idx:(self.sizeWindow + idx)].view(1, -1)\n",
        "        label = torch.tensor(self.getCategoryLabel(idx), dtype=torch.long)\n",
        "        return outData, label\n",
        "\n",
        "    def getBaseSampler(self, samplingType, batchSize, offset):\n",
        "        if samplingType == \"samecategory\":\n",
        "            return SameTrackSampler(batchSize, self.categoryLabel, self.sizeWindow, offset)\n",
        "        if samplingType == \"samesequence\":\n",
        "            return SameTrackSampler(batchSize, self.seqLabel, self.sizeWindow, offset)\n",
        "        if samplingType == \"sequential\":\n",
        "            return SequentialSampler(len(self.data), self.sizeWindow, offset, batchSize)\n",
        "\n",
        "        sampler = UniformAudioSampler(len(self.data), self.sizeWindow, offset)\n",
        "        return BatchSampler(sampler, batchSize, True)\n",
        "\n",
        "    def getDataLoader(self, batchSize, samplingType, randomOffset, numWorkers=0,\n",
        "                      onLoop=-1):\n",
        "        r\"\"\"\n",
        "        Get a batch sampler for the current dataset.\n",
        "            - batchSize (int): batch size\n",
        "            - groupSize (int): in the case of type in [\"track\", \"sequence\"]\n",
        "            number of items sharing a same label in the group\n",
        "            (see AudioBatchSampler)\n",
        "            - type (string):\n",
        "                type == \"track\": grouped sampler track-wise\n",
        "                type == \"sequence\": grouped sampler sequence-wise\n",
        "                type == \"sequential\": sequential sampling\n",
        "                else: uniform random sampling of the full audio\n",
        "                vector\n",
        "            - randomOffset (bool): if True add a random offset to the sampler\n",
        "                                   at the begining of each iteration\n",
        "        \"\"\"\n",
        "        nLoops = len(self.packs)\n",
        "        totSize = self.totSize // (self.sizeWindow * batchSize)\n",
        "        if onLoop >= 0:\n",
        "            self.currentPack = onLoop - 1\n",
        "            self._loadNextPack()\n",
        "            nLoops = 1\n",
        "\n",
        "        def samplerCall():\n",
        "            offset = random.randint(0, self.sizeWindow // 2) \\\n",
        "                if randomOffset else 0\n",
        "            return self.getBaseSampler(samplingType, batchSize, offset)\n",
        "\n",
        "        return AudioLoader(self, samplerCall, nLoops, self._loadNextPack, totSize, numWorkers)\n",
        "\n",
        "\n",
        "class AudioLoader(object):\n",
        "    r\"\"\"\n",
        "    A DataLoader meant to handle an AudioBatchData object.\n",
        "    In order to handle big datasets AudioBatchData works with big chunks of\n",
        "    audio it loads sequentially in memory: once all batches have been sampled\n",
        "    on a chunk, the AudioBatchData loads the next one.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 samplerCall,\n",
        "                 nLoop,\n",
        "                 updateCall,\n",
        "                 size,\n",
        "                 numWorkers):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            - dataset (AudioBatchData): target dataset\n",
        "            - samplerCall (function): batch-sampler to call\n",
        "            - nLoop (int): number of chunks to load\n",
        "            - updateCall (function): function loading the next chunk\n",
        "            - size (int): total number of batches\n",
        "            - numWorkers (int): see torch.utils.data.DataLoader\n",
        "        \"\"\"\n",
        "        self.samplerCall = samplerCall\n",
        "        self.updateCall = updateCall\n",
        "        self.nLoop = nLoop\n",
        "        self.size = size\n",
        "        self.dataset = dataset\n",
        "        self.numWorkers = numWorkers\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        for i in range(self.nLoop):\n",
        "            sampler = self.samplerCall()\n",
        "            dataloader = DataLoader(self.dataset,\n",
        "                                    batch_sampler=sampler,\n",
        "                                    num_workers=self.numWorkers)\n",
        "            # print(\"Data loader nLoop: \", self.nLoop)\n",
        "            # print(\"Len data loader: \", len(dataloader))\n",
        "            # print(\"Len of sampler: \", len(sampler))\n",
        "            # assert False\n",
        "            # print(\"Dataloader len: \\n\", len(dataloader))\n",
        "            for j, x in enumerate(dataloader):\n",
        "                # print(\"Data loader yielded batch #: \", j)\n",
        "                yield x\n",
        "            # print(\"Len data loader: \", len(dataloader), \"and consummed: \", j + 1)\n",
        "            if i < self.nLoop - 1:\n",
        "                self.updateCall()\n",
        "\n",
        "\n",
        "class UniformAudioSampler(Sampler):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataSize,\n",
        "                 sizeWindow,\n",
        "                 offset):\n",
        "        self.len = dataSize // sizeWindow\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.offset = offset\n",
        "        if self.offset > 0:\n",
        "            self.len -= 1\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter((self.offset\n",
        "                     + self.sizeWindow * torch.randperm(self.len)).tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class SequentialSampler(Sampler):\n",
        "\n",
        "    def __init__(self, dataSize, sizeWindow, offset, batchSize):\n",
        "\n",
        "        self.len = (dataSize // sizeWindow) // batchSize\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.offset = offset\n",
        "        self.startBatches = [x * (dataSize // batchSize)\n",
        "                             for x in range(batchSize)]\n",
        "        self.batchSize = batchSize\n",
        "        if self.offset > 0:\n",
        "            self.len -= 1\n",
        "\n",
        "    def __iter__(self):\n",
        "        for idx in range(self.len):\n",
        "            yield [self.offset + self.sizeWindow * idx\n",
        "                   + start for start in self.startBatches]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "class SameTrackSampler(Sampler):\n",
        "\n",
        "    def __init__(self,\n",
        "                 batchSize,\n",
        "                 samplingIntervals,\n",
        "                 sizeWindow,\n",
        "                 offset):\n",
        "\n",
        "        self.samplingIntervals = samplingIntervals\n",
        "        self.sizeWindow = sizeWindow\n",
        "        self.batchSize = batchSize\n",
        "        self.offset = offset\n",
        "\n",
        "        if self.samplingIntervals[0] != 0:\n",
        "            raise AttributeError(\"Sampling intervals should start at zero\")\n",
        "\n",
        "        # print(\"Sampling intervals:\\n\", self.samplingIntervals)\n",
        "        nWindows = len(self.samplingIntervals) - 1\n",
        "        self.sizeSamplers = [(self.samplingIntervals[i + 1] -\n",
        "                              self.samplingIntervals[i]) // self.sizeWindow\n",
        "                             for i in range(nWindows)]  # How many windows a sequence/category lasts \n",
        "\n",
        "        # assert False\n",
        "        if self.offset > 0:\n",
        "            self.sizeSamplers = [max(0, x - 1) for x in self.sizeSamplers]\n",
        "        # print(\"Size samplers:\\n\", self.sizeSamplers)\n",
        "        # print(\"Size samplers over batch size:\\n\", np.array(self.sizeSamplers) // self.batchSize)\n",
        "\n",
        "        order = [(x, torch.randperm(val).tolist())\n",
        "                 for x, val in enumerate(self.sizeSamplers) if\n",
        "                 val > 0]  # (index of seq/cat, randomly permuted numbers from 0 to num windows in seq(cat))\n",
        "\n",
        "        # Build Batches\n",
        "        self.batches = []\n",
        "        for indexSampler, randperm in order:\n",
        "            indexStart, sizeSampler = 0, self.sizeSamplers[indexSampler]\n",
        "            while indexStart < (sizeSampler - self.batchSize):\n",
        "                indexEnd = indexStart + self.batchSize\n",
        "                locBatch = [self.getIndex(x, indexSampler)\n",
        "                            for x in randperm[indexStart:indexEnd]]\n",
        "                indexStart = indexEnd\n",
        "                self.batches.append(locBatch)\n",
        "        # print(\"Number of batches:\\n\", len(self.batches))\n",
        "        # print(\"Batches:\\n\", self.batches)\n",
        "        # print(\"Batches shape: \\n\", np.array(self.batches).shape)\n",
        "        # print(\"Batches vstack shape: \\n\", np.vstack(self.batches).shape)\n",
        "        self.batches = np.vstack(self.batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def getIndex(self, x, iInterval):\n",
        "        return self.offset + x * self.sizeWindow + self.samplingIntervals[iInterval]\n",
        "\n",
        "    def __iter__(self):\n",
        "        random.shuffle(self.batches)\n",
        "        return iter(self.batches)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMMWgc3M4xln"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbjSx6SH4yo1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class ChannelNorm(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 numFeatures,\n",
        "                 epsilon=1e-05,\n",
        "                 affine=True):\n",
        "\n",
        "        super(ChannelNorm, self).__init__()\n",
        "        if affine:\n",
        "            self.weight = nn.parameter.Parameter(torch.Tensor(1,\n",
        "                                                              numFeatures, 1))\n",
        "            self.bias = nn.parameter.Parameter(torch.Tensor(1, numFeatures, 1))\n",
        "        else:\n",
        "            self.weight = None\n",
        "            self.bias = None\n",
        "        self.epsilon = epsilon\n",
        "        self.p = 0\n",
        "        self.affine = affine\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.affine:\n",
        "            torch.nn.init.ones_(self.weight)\n",
        "            torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        cumMean = x.mean(dim=1, keepdim=True)\n",
        "        cumVar = x.var(dim=1, keepdim=True)\n",
        "        x = (x - cumMean) * torch.rsqrt(cumVar + self.epsilon)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            x = x * self.weight + self.bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class SincConv1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, \n",
        "                 padding_mode='zeros', sampleRate=16000, minLowHz=50, minBandHz=50):\n",
        "        super(SincConv1D, self).__init__()\n",
        "        if in_channels != 1:\n",
        "            msg = \"SincConv1D only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
        "            raise ValueError(msg)\n",
        "        self.outChannels = out_channels\n",
        "        self.kernelSize = kernel_size\n",
        "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
        "        if self.kernelSize % 2 == 0:\n",
        "            self.kernelSize += 1\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        if bias:\n",
        "            raise ValueError('SincConv1D does not support bias.')\n",
        "        if groups > 1:\n",
        "            raise ValueError('SincConv1D does not support groups.')\n",
        "        self.sampleRate = sampleRate\n",
        "        self.minLowHz = minLowHz\n",
        "        self.minBandHz = minBandHz\n",
        "        # Initialize filterbanks such that they are equally spaced in Mel scale\n",
        "        lowHz = 30\n",
        "        highHz = self.sampleRate / 2 - (self.minLowHz + self.minBandHz)\n",
        "        mel = np.linspace(self.hz2Mel(lowHz), self.hz2Mel(highHz), self.outChannels + 1)\n",
        "        hz = self.mel2Hz(mel)\n",
        "        # Filter lower frequency (outChannels, 1)\n",
        "        self.lowHz_ = torch.nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
        "        # Filter frequency band (outChannels, 1)\n",
        "        self.bandHz_ = torch.nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
        "        # Hamming window\n",
        "        nLin= torch.linspace(0, (self.kernelSize / 2) - 1, \n",
        "                             steps=int((self.kernelSize / 2))) # computing only half of the window\n",
        "        self.window_ = 0.54 - 0.46 * torch.cos(2 * math.pi * nLin / self.kernelSize);\n",
        "        n = (self.kernelSize - 1) / 2.0\n",
        "        self.n_ = 2 * math.pi * torch.arange(-n, 0).view(1, -1) / self.sampleRate # Due to symmetry, we only need half of the time axes\n",
        "    \n",
        "    @staticmethod\n",
        "    def hz2Mel(hz):\n",
        "        return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "    @staticmethod\n",
        "    def mel2Hz(mel):\n",
        "        return 700 * (10 ** (mel / 2595) - 1)\n",
        "\n",
        "    def forward(self, waveforms):\n",
        "        self.n_ = self.n_.to(waveforms.device)\n",
        "        self.window_ = self.window_.to(waveforms.device)\n",
        "        low = self.minLowHz  + torch.abs(self.lowHz_)\n",
        "        high = torch.clamp(low + self.minBandHz + torch.abs(self.bandHz_), self.minLowHz, self.sampleRate/2)\n",
        "        band = (high - low)[:, 0]\n",
        "        fTimesTLow = torch.matmul(low, self.n_)\n",
        "        fTimesTHigh = torch.matmul(high, self.n_)\n",
        "        # Equivalent of Eq.4 of the reference paper\n",
        "        bandPassLeft = ((torch.sin(fTimesTHigh) - torch.sin(fTimesTLow)) / (self.n_/2)) * self.window_ \n",
        "        bandPassCenter = 2 * band.view(-1, 1)\n",
        "        bandPassRight = torch.flip(bandPassLeft, dims=[1])\n",
        "        bandPass = torch.cat([bandPassLeft, bandPassCenter, bandPassRight], dim=1)\n",
        "        bandPass = bandPass / (2 * band[:, None])\n",
        "        self.filters = (bandPass).view(self.outChannels, 1, self.kernelSize)\n",
        "        return torch.conv1d(waveforms, self.filters, stride=self.stride, padding=self.padding, \n",
        "                            dilation=self.dilation, bias=None, groups=1) \n",
        "\n",
        "\n",
        "class SincNetEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sizeHidden=512,\n",
        "                 normMode=\"layerNorm\"):\n",
        "        super(SincNetEncoder, self).__init__()     \n",
        "        normLayer = ChannelNorm\n",
        "        self.dimEncoded = sizeHidden\n",
        "        self.conv0 = SincConv1D(1, sizeHidden, 10, stride=5, padding=3)\n",
        "        self.batchNorm0 = normLayer(sizeHidden)\n",
        "\n",
        "\n",
        "\n",
        "class CPCEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 sizeHidden=512,\n",
        "                 normMode=\"layerNorm\", sincNet=False):\n",
        "\n",
        "        super(CPCEncoder, self).__init__()\n",
        "\n",
        "        validModes = [\"batchNorm\", \"instanceNorm\", \"ID\", \"layerNorm\"]\n",
        "        if normMode not in validModes:\n",
        "            raise ValueError(f\"Norm mode must be in {validModes}\")\n",
        "\n",
        "        if normMode == \"instanceNorm\":\n",
        "            def normLayer(x):\n",
        "                return nn.InstanceNorm1d(x, affine=True)\n",
        "        elif normMode == \"layerNorm\":\n",
        "            normLayer = ChannelNorm\n",
        "        else:\n",
        "            normLayer = nn.BatchNorm1d\n",
        "\n",
        "        self.dimEncoded = sizeHidden\n",
        "        if sincNet:\n",
        "            self.conv0 = SincConv1D(1, sizeHidden, 10, stride=5, padding=3)\n",
        "        else:\n",
        "            self.conv0 = nn.Conv1d(1, sizeHidden, 10, stride=5, padding=3)\n",
        "        self.batchNorm0 = normLayer(sizeHidden)\n",
        "        self.conv1 = nn.Conv1d(sizeHidden, sizeHidden, 8, stride=4, padding=2)\n",
        "        self.batchNorm1 = normLayer(sizeHidden)\n",
        "        self.conv2 = nn.Conv1d(sizeHidden, sizeHidden, 4,\n",
        "                               stride=2, padding=1)\n",
        "        self.batchNorm2 = normLayer(sizeHidden)\n",
        "        self.conv3 = nn.Conv1d(sizeHidden, sizeHidden, 4, stride=2, padding=1)\n",
        "        self.batchNorm3 = normLayer(sizeHidden)\n",
        "        self.conv4 = nn.Conv1d(sizeHidden, sizeHidden, 4, stride=2, padding=1)\n",
        "        self.batchNorm4 = normLayer(sizeHidden)\n",
        "        self.DOWNSAMPLING = 160\n",
        "\n",
        "    def getDimOutput(self):\n",
        "        return self.conv4.out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.batchNorm0(self.conv0(x)))\n",
        "        x = torch.relu(self.batchNorm1(self.conv1(x)))\n",
        "        x = torch.relu(self.batchNorm2(self.conv2(x)))\n",
        "        x = torch.relu(self.batchNorm3(self.conv3(x)))\n",
        "        x = torch.relu(self.batchNorm4(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CPCAR(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dimEncoded,\n",
        "                 dimOutput,\n",
        "                 keepHidden,\n",
        "                 nLevelsGRU,\n",
        "                 mode=\"GRU\",\n",
        "                 reverse=False):\n",
        "\n",
        "        super(CPCAR, self).__init__()\n",
        "        self.RESIDUAL_STD = 0.1\n",
        "\n",
        "        if mode == \"LSTM\":\n",
        "            self.baseNet = nn.LSTM(dimEncoded, dimOutput,\n",
        "                                   num_layers=nLevelsGRU, batch_first=True)\n",
        "        elif mode == \"RNN\":\n",
        "            self.baseNet = nn.RNN(dimEncoded, dimOutput,\n",
        "                                  num_layers=nLevelsGRU, batch_first=True)\n",
        "        else:\n",
        "            self.baseNet = nn.GRU(dimEncoded, dimOutput,\n",
        "                                  num_layers=nLevelsGRU, batch_first=True)\n",
        "\n",
        "        self.hidden = None\n",
        "        self.keepHidden = keepHidden\n",
        "        self.reverse = reverse\n",
        "\n",
        "    def getDimOutput(self):\n",
        "        return self.baseNet.hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.reverse:\n",
        "            x = torch.flip(x, [1])\n",
        "        try:\n",
        "            self.baseNet.flatten_parameters()\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "        x, h = self.baseNet(x, self.hidden)\n",
        "        if self.keepHidden:\n",
        "            if isinstance(h, tuple):\n",
        "                self.hidden = tuple(x.detach() for x in h)\n",
        "            else:\n",
        "                self.hidden = h.detach()\n",
        "\n",
        "        # For better modularity, a sequence's order should be preserved\n",
        "        # by each module\n",
        "        if self.reverse:\n",
        "            x = torch.flip(x, [1])\n",
        "        return x\n",
        "\n",
        "\n",
        "class CPCModel(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 AR):\n",
        "        super(CPCModel, self).__init__()\n",
        "        self.gEncoder = encoder\n",
        "        self.gAR = AR\n",
        "\n",
        "    def forward(self, batchData, label):\n",
        "        encodedData = self.gEncoder(batchData).permute(0, 2, 1)\n",
        "        cFeature = self.gAR(encodedData)\n",
        "        return cFeature, encodedData, label\n",
        "\n",
        "\n",
        "class PredictionNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nPredicts,\n",
        "                 dimOutputAR,\n",
        "                 dimOutputEncoder,\n",
        "                 dropout=False):\n",
        "\n",
        "        super(PredictionNetwork, self).__init__()\n",
        "        self.predictors = nn.ModuleList()\n",
        "        self.RESIDUAL_STD = 0.01\n",
        "        self.dimOutputAR = dimOutputAR\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5) if dropout else None\n",
        "        for i in range(nPredicts):\n",
        "            self.predictors.append(\n",
        "                nn.Linear(dimOutputAR, dimOutputEncoder, bias=False))\n",
        "            if dimOutputEncoder > dimOutputAR:\n",
        "                residual = dimOutputEncoder - dimOutputAR\n",
        "                self.predictors[-1].weight.data.copy_(torch.cat([torch.randn(\n",
        "                    dimOutputAR, dimOutputAR), self.RESIDUAL_STD * torch.randn(residual, dimOutputAR)], dim=0))\n",
        "\n",
        "    def forward(self, c, candidates):\n",
        "\n",
        "        assert (len(candidates) == len(self.predictors))\n",
        "        out = []\n",
        "\n",
        "        # UGLY\n",
        "        # if isinstance(self.predictors[0], EqualizedConv1d):\n",
        "        # c = c.permute(0, 2, 1)\n",
        "\n",
        "        for k in range(len(self.predictors)):\n",
        "\n",
        "            locC = self.predictors[k](c)\n",
        "            if isinstance(locC, tuple):\n",
        "                locC = locC[0]\n",
        "            # if isinstance(self.predictors[k], EqualizedConv1d):\n",
        "            # locC = locC.permute(0, 2, 1)\n",
        "            if self.dropout is not None:\n",
        "                locC = self.dropout(locC)\n",
        "            locC = locC.view(locC.size(0), 1, locC.size(1), locC.size(2))\n",
        "            outK = (locC * candidates[k]).mean(dim=3)\n",
        "            out.append(outK)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BaseCriterion(nn.Module):\n",
        "    def update(self):\n",
        "        return\n",
        "\n",
        "\n",
        "class CPCUnsupersivedCriterion(BaseCriterion):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nPredicts,  # Number of steps\n",
        "                 dimOutputAR,  # Dimension of G_ar\n",
        "                 dimOutputEncoder,  # Dimension of the convolutional net\n",
        "                 negativeSamplingExt,  # Number of negative samples to draw\n",
        "                 mode=None,\n",
        "                 dropout=False):\n",
        "\n",
        "        super(CPCUnsupersivedCriterion, self).__init__()\n",
        "\n",
        "        self.wPrediction = PredictionNetwork(\n",
        "            nPredicts, dimOutputAR, dimOutputEncoder, dropout=dropout)\n",
        "        self.nPredicts = nPredicts\n",
        "        self.negativeSamplingExt = negativeSamplingExt\n",
        "        self.lossCriterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        if mode not in [None, \"reverse\"]:\n",
        "            raise ValueError(\"Invalid mode\")\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "    def sampleClean(self, encodedData, windowSize):\n",
        "\n",
        "        batchSize, nNegativeExt, dimEncoded = encodedData.size()\n",
        "        outputs = []\n",
        "\n",
        "        negExt = encodedData.contiguous().view(-1, dimEncoded)\n",
        "        # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
        "        batchIdx = torch.randint(low=0, high=batchSize,\n",
        "                                 size=(self.negativeSamplingExt\n",
        "                                       * windowSize * batchSize,),\n",
        "                                 device=encodedData.device)\n",
        "\n",
        "        seqIdx = torch.randint(low=1, high=nNegativeExt,\n",
        "                               size=(self.negativeSamplingExt\n",
        "                                     * windowSize * batchSize,),\n",
        "                               device=encodedData.device)\n",
        "\n",
        "        baseIdx = torch.arange(0, windowSize, device=encodedData.device)\n",
        "        baseIdx = baseIdx.view(1, 1,\n",
        "                               windowSize).expand(1,\n",
        "                                                  self.negativeSamplingExt,\n",
        "                                                  windowSize).expand(batchSize, self.negativeSamplingExt, windowSize)\n",
        "        seqIdx += baseIdx.contiguous().view(-1)\n",
        "        seqIdx = torch.remainder(seqIdx, nNegativeExt)\n",
        "\n",
        "        extIdx = seqIdx + batchIdx * nNegativeExt\n",
        "        negExt = negExt[extIdx].view(batchSize, self.negativeSamplingExt,\n",
        "                                     windowSize, dimEncoded)\n",
        "\n",
        "        labelLoss = torch.zeros((batchSize * windowSize),\n",
        "                                dtype=torch.long,\n",
        "                                device=encodedData.device)\n",
        "\n",
        "        for k in range(1, self.nPredicts + 1):\n",
        "\n",
        "            # Positive samples\n",
        "            if k < self.nPredicts:\n",
        "                posSeq = encodedData[:, k:-(self.nPredicts - k)]\n",
        "            else:\n",
        "                posSeq = encodedData[:, k:]\n",
        "\n",
        "            posSeq = posSeq.view(batchSize, 1, windowSize, dimEncoded)\n",
        "            fullSeq = torch.cat((posSeq, negExt), dim=1)\n",
        "            outputs.append(fullSeq)\n",
        "\n",
        "        return outputs, labelLoss\n",
        "\n",
        "    def getInnerLoss(self):\n",
        "\n",
        "        return \"orthoLoss\", self.orthoLoss * self.wPrediction.orthoCriterion()\n",
        "\n",
        "    def forward(self, cFeature, encodedData):\n",
        "\n",
        "        if self.mode == \"reverse\":\n",
        "            encodedData = torch.flip(encodedData, [1])\n",
        "            cFeature = torch.flip(cFeature, [1])\n",
        "\n",
        "        batchSize, seqSize, dimAR = cFeature.size()\n",
        "        windowSize = seqSize - self.nPredicts\n",
        "\n",
        "        cFeature = cFeature[:, :windowSize]\n",
        "\n",
        "        sampledData, labelLoss = self.sampleClean(encodedData, windowSize)\n",
        "\n",
        "        predictions = self.wPrediction(cFeature, sampledData)\n",
        "\n",
        "        outLosses = [0 for _ in range(self.nPredicts)]\n",
        "        outAcc = [0 for _ in range(self.nPredicts)]\n",
        "\n",
        "        for k, locPreds in enumerate(predictions[:self.nPredicts]):\n",
        "            locPreds = locPreds.permute(0, 2, 1)  # (batchSize, 1 + negativeSamplingExt, windowSize) to\n",
        "            #                                       (batchSize, windowSize, 1 + negativeSamplingExt)\n",
        "            locPreds = locPreds.contiguous().view(\n",
        "                -1, locPreds.size(2))  # (batchSize, windowSize, 1 + negativeSamplingExt) to\n",
        "            #                            (batchSize * windowSize, 1 + negativeSamplingExt)\n",
        "            lossK = self.lossCriterion(locPreds, labelLoss)\n",
        "            outLosses[k] += lossK.view(1, -1)\n",
        "            _, predsIndex = locPreds.max(1)\n",
        "            outAcc[k] += torch.sum(predsIndex == labelLoss).float().view(1, -1)\n",
        "\n",
        "        return torch.cat(outLosses, dim=1), torch.cat(outAcc, dim=1) / (windowSize * batchSize)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_rcKRYDQDHW"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZR5tuFYQCeJ"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sizeSeq,         # Size of the input sequence\n",
        "                 dk,              # Dimension of the input sequence\n",
        "                 dropout,         # Dropout parameter\n",
        "                 relpos=False):   # Do we retrieve positional information ?\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        self.relpos = relpos\n",
        "        self.sizeSeq = sizeSeq\n",
        "\n",
        "        if relpos:\n",
        "            self.Krelpos = nn.Parameter(torch.Tensor(dk, sizeSeq))\n",
        "            self.initmat_(self.Krelpos)\n",
        "            self.register_buffer('z', torch.zeros(1, sizeSeq, 1))\n",
        "\n",
        "        # A mask is set so that a node never queries data in the future\n",
        "        mask = torch.tril(torch.ones(sizeSeq, sizeSeq), diagonal=0)\n",
        "        mask = 1 - mask\n",
        "        mask[mask == 1] = -float('inf')\n",
        "        self.register_buffer('mask', mask.unsqueeze(0))\n",
        "\n",
        "    def initmat_(self, mat, dim=0):\n",
        "        stdv = 1. / math.sqrt(mat.size(dim))\n",
        "        mat.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        # Input dim : N x sizeSeq x dk\n",
        "        QK = torch.bmm(Q, K.transpose(-2, -1))\n",
        "\n",
        "        if self.relpos:\n",
        "            bsz = Q.size(0)\n",
        "            QP = Q.matmul(self.Krelpos)\n",
        "            # This trick with z fills QP's diagonal with zeros\n",
        "            QP = torch.cat((self.z.expand(bsz, -1, -1), QP), 2)\n",
        "            QK += QP.view(bsz, self.sizeSeq + 1, self.sizeSeq)[:, 1:, :]\n",
        "        A = self.softmax(QK / math.sqrt(K.size(-1)) + self.mask)\n",
        "        return torch.bmm(self.drop(A), V)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sizeSeq,   # Size of a sequence\n",
        "                 dropout,   # Dropout parameter\n",
        "                 dmodel,    # Model's dimension\n",
        "                 nheads,    # Number of heads in the model\n",
        "                 abspos):   # Is positional information encoded in the input ?\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.Wo = nn.Linear(dmodel, dmodel, bias=False)\n",
        "        self.Wk = nn.Linear(dmodel, dmodel, bias=False)\n",
        "        self.Wq = nn.Linear(dmodel, dmodel, bias=False)\n",
        "        self.Wv = nn.Linear(dmodel, dmodel, bias=False)\n",
        "        self.nheads = nheads\n",
        "        self.dk = dmodel // nheads\n",
        "        self.Att = ScaledDotProductAttention(sizeSeq, self.dk,\n",
        "                                             dropout, not abspos)\n",
        "\n",
        "    def trans_(self, x):\n",
        "        bsz, bptt, h, dk = x.size(0), x.size(1), self.nheads, self.dk\n",
        "        return x.view(bsz, bptt, h, dk).transpose(1, 2).contiguous().view(bsz * h, bptt, dk)\n",
        "\n",
        "    def reverse_trans_(self, x):\n",
        "        bsz, bptt, h, dk = x.size(\n",
        "            0) // self.nheads, x.size(1), self.nheads, self.dk\n",
        "        return x.view(bsz, h, bptt, dk).transpose(1, 2).contiguous().view(bsz, bptt, h * dk)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        q = self.trans_(self.Wq(Q))\n",
        "        k = self.trans_(self.Wk(K))\n",
        "        v = self.trans_(self.Wv(V))\n",
        "        y = self.reverse_trans_(self.Att(q, k, v))\n",
        "        return self.Wo(y)\n",
        "\n",
        "\n",
        "class FFNetwork(nn.Module):\n",
        "    def __init__(self, din, dout, dff, dropout):\n",
        "        super(FFNetwork, self).__init__()\n",
        "        self.lin1 = nn.Linear(din, dff, bias=True)\n",
        "        self.lin2 = nn.Linear(dff, dout, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.drop(self.relu(self.lin1(x))))\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, sizeSeq=32, dmodel=512, dff=2048,\n",
        "                 dropout=0.1, nheads=8,\n",
        "                 abspos=False):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.multihead = MultiHeadAttention(sizeSeq, dropout,\n",
        "                                            dmodel, nheads, abspos)\n",
        "        self.ln_multihead = nn.LayerNorm(dmodel)\n",
        "        self.ffnetwork = FFNetwork(dmodel, dmodel, dff, dropout)\n",
        "        self.ln_ffnetwork = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.ln_multihead(x + self.multihead(Q=x, K=x, V=x))\n",
        "        return self.ln_ffnetwork(y + self.ffnetwork(y))\n",
        "\n",
        "\n",
        "class StaticPositionEmbedding(nn.Module):\n",
        "    def __init__(self, seqlen, dmodel):\n",
        "        super(StaticPositionEmbedding, self).__init__()\n",
        "        pos = torch.arange(0., seqlen).unsqueeze(1).repeat(1, dmodel)\n",
        "        dim = torch.arange(0., dmodel).unsqueeze(0).repeat(seqlen, 1)\n",
        "        div = torch.exp(- math.log(10000) * (2*(dim//2)/dmodel))\n",
        "        pos *= div\n",
        "        pos[:, 0::2] = torch.sin(pos[:, 0::2])\n",
        "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
        "        self.register_buffer('pe', pos.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "def buildTransformerAR(dimEncoded,    # Output dimension of the encoder\n",
        "                       nLayers,       # Number of transformer layers\n",
        "                       sizeSeq,       # Expected size of the input sequence\n",
        "                       abspos):\n",
        "    layerSequence = []\n",
        "    if abspos:\n",
        "        layerSequence += [StaticPositionEmbedding(sizeSeq, dimEncoded)]\n",
        "    layerSequence += [TransformerLayer(sizeSeq=sizeSeq,\n",
        "                                       dmodel=dimEncoded, abspos=abspos)\n",
        "                      for i in range(nLayers)]\n",
        "    return nn.Sequential(*layerSequence)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJSRs0F43UA"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtqKVS-444yK"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from copy import deepcopy\n",
        "# import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "\n",
        "def update_logs(logs, logStep, prevlogs=None):\n",
        "    out = {}\n",
        "    for key in logs:\n",
        "        out[key] = deepcopy(logs[key])\n",
        "\n",
        "        if prevlogs is not None:\n",
        "            out[key] -= prevlogs[key]\n",
        "        out[key] /= logStep\n",
        "    return out\n",
        "\n",
        "\n",
        "def save_logs(data, pathLogs):\n",
        "    with open(pathLogs, 'w') as file:\n",
        "        json.dump(data, file, indent=2)\n",
        "\n",
        "\n",
        "def save_checkpoint(model_state, criterion_state, optimizer_state, best_state,\n",
        "                    path_checkpoint):\n",
        "\n",
        "    state_dict = {\"gEncoder\": model_state,\n",
        "                  \"cpcCriterion\": criterion_state,\n",
        "                  \"optimizer\": optimizer_state,\n",
        "                  \"best\": best_state}\n",
        "\n",
        "    torch.save(state_dict, path_checkpoint)\n",
        "\n",
        "\n",
        "def show_logs(text, logs):\n",
        "    print(\"\")\n",
        "    print('-' * 50)\n",
        "    print(text)\n",
        "\n",
        "    for key in logs:\n",
        "\n",
        "        if key == \"iter\":\n",
        "            continue\n",
        "\n",
        "        nPredicts = logs[key].shape[0]\n",
        "\n",
        "        strSteps = ['Step'] + [str(s) for s in range(1, nPredicts + 1)]\n",
        "        formatCommand = ' '.join(['{:>16}' for _ in range(nPredicts + 1)])\n",
        "        print(formatCommand.format(*strSteps))\n",
        "\n",
        "        strLog = [key] + [\"{:10.6f}\".format(s) for s in logs[key]]\n",
        "        print(formatCommand.format(*strLog))\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "def trainStep(dataLoader,\n",
        "              cpcModel,\n",
        "              cpcCriterion,\n",
        "              optimizer,\n",
        "              loggingStep,\n",
        "              useGPU,\n",
        "              log2Board=0,\n",
        "              totalSteps=0):\n",
        "    cpcModel.train()\n",
        "    cpcCriterion.train()\n",
        "\n",
        "    startTime = time.perf_counter()\n",
        "    n_examples = 0\n",
        "    logs, lastlogs = {}, None\n",
        "    iterCtr = 0\n",
        "\n",
        "    if log2Board > 1:\n",
        "        gradmapGEncoder = {}\n",
        "        gradmapGAR = {}\n",
        "        gradmapWPrediction = {}\n",
        "        if totalSteps == 0:\n",
        "            logWeights(cpcModel.gEncoder, totalSteps)\n",
        "            logWeights(cpcModel.gAR, totalSteps)\n",
        "            logWeights(cpcCriterion.wPrediction, totalSteps)\n",
        "\n",
        "    for step, fulldata in enumerate(dataLoader):\n",
        "        batchData, label = fulldata\n",
        "        n_examples += batchData.size(0)\n",
        "        if useGPU:\n",
        "            batchData = batchData.cuda(non_blocking=True)\n",
        "            label = label.cuda(non_blocking=True)\n",
        "        c_feature, encoded_data, label = cpcModel(batchData, label)\n",
        "        allLosses, allAcc = cpcCriterion(c_feature, encoded_data)\n",
        "        totLoss = allLosses.sum()\n",
        "\n",
        "        totLoss.backward()\n",
        "\n",
        "        if log2Board > 1:\n",
        "            gradmapGEncoder = updateGradientMap(cpcModel.gEncoder, gradmapGEncoder)\n",
        "            gradmapGAR = updateGradientMap(cpcModel.gAR, gradmapGAR)\n",
        "            gradmapWPrediction = updateGradientMap(cpcCriterion.wPrediction, gradmapWPrediction)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if \"locLoss_train\" not in logs:\n",
        "            logs[\"locLoss_train\"] = np.zeros(allLosses.size(1))\n",
        "            logs[\"locAcc_train\"] = np.zeros(allLosses.size(1))\n",
        "\n",
        "        logs[\"locLoss_train\"] += (allLosses.mean(dim=0)).detach().cpu().numpy()\n",
        "        logs[\"locAcc_train\"] += (allAcc.mean(dim=0)).cpu().numpy()\n",
        "        iterCtr += 1\n",
        "\n",
        "        if log2Board:\n",
        "            for t in range(len(logs[\"locLoss_train\"])):\n",
        "                experiment.log_metric(f\"Losses/batch/locLoss_train_{t}\", logs[\"locLoss_train\"][t] / iterCtr, step=totalSteps + iterCtr)\n",
        "                experiment.log_metric(f\"Accuracy/batch/locAcc_train_{t}\", logs[\"locAcc_train\"][t] / iterCtr, step=totalSteps + iterCtr)\n",
        "\n",
        "        if (step + 1) % loggingStep == 0:\n",
        "            new_time = time.perf_counter()\n",
        "            elapsed = new_time - startTime\n",
        "            print(f\"Update {step + 1}\")\n",
        "            print(f\"elapsed: {elapsed:.1f} s\")\n",
        "            print(\n",
        "                f\"{1000.0 * elapsed / loggingStep:.1f} ms per batch, {1000.0 * elapsed / n_examples:.1f} ms / example\")\n",
        "            locLogs = update_logs(logs, loggingStep, lastlogs)\n",
        "            lastlogs = deepcopy(logs)\n",
        "            show_logs(\"Training loss\", locLogs)\n",
        "            startTime, n_examples = new_time, 0\n",
        "            \n",
        "            if log2Board > 1:\n",
        "                # Log gradients\n",
        "                logGradients(gradmapGEncoder, totalSteps + iterCtr, scaleBy=1.0 / iterCtr)\n",
        "                logGradients(gradmapGAR, totalSteps + iterCtr, scaleBy=1.0 / iterCtr)\n",
        "                logGradients(gradmapWPrediction, totalSteps + iterCtr, scaleBy=1.0 / iterCtr)\n",
        "                # Log weights\n",
        "                logWeights(cpcModel.gEncoder, totalSteps + iterCtr)\n",
        "                logWeights(cpcModel.gAR, totalSteps + iterCtr)\n",
        "                logWeights(cpcCriterion.wPrediction, totalSteps + iterCtr)\n",
        "\n",
        "    logs = update_logs(logs, iterCtr)\n",
        "    logs[\"iter\"] = iterCtr\n",
        "    show_logs(\"Average training loss on epoch\", logs)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def valStep(dataLoader,\n",
        "            cpcModel,\n",
        "            cpcCriterion,\n",
        "            useGPU):\n",
        "    cpcCriterion.eval()\n",
        "    cpcModel.eval()\n",
        "    logs = {}\n",
        "    cpcCriterion.eval()\n",
        "    cpcModel.eval()\n",
        "    iterCtr = 0\n",
        "\n",
        "    for step, fulldata in enumerate(dataLoader):\n",
        "\n",
        "        batchData, label = fulldata\n",
        "\n",
        "        if useGPU:\n",
        "            batchData = batchData.cuda(non_blocking=True)\n",
        "            label = label.cuda(non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            c_feature, encoded_data, label = cpcModel(batchData, label)\n",
        "            allLosses, allAcc = cpcCriterion(c_feature, encoded_data)\n",
        "\n",
        "        if \"locLoss_val\" not in logs:\n",
        "            logs[\"locLoss_val\"] = np.zeros(allLosses.size(1))\n",
        "            logs[\"locAcc_val\"] = np.zeros(allLosses.size(1))\n",
        "\n",
        "        iterCtr += 1\n",
        "        logs[\"locLoss_val\"] += allLosses.mean(dim=0).cpu().numpy()\n",
        "        logs[\"locAcc_val\"] += allAcc.mean(dim=0).cpu().numpy()\n",
        "\n",
        "    logs = update_logs(logs, iterCtr)\n",
        "    logs[\"iter\"] = iterCtr\n",
        "    show_logs(\"Validation loss:\", logs)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def updateGradientMap(model, gradMap):\n",
        "    for name, param in model.named_parameters():\n",
        "        paramName = name.split('.')\n",
        "        paramLabel = paramName[-1]\n",
        "        if paramLabel not in ['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0', \n",
        "                              'lowHz_', 'bandHz_', 'weight', 'bias']:\n",
        "            continue\n",
        "        param = model\n",
        "        for i in range(len(paramName)):\n",
        "            param = getattr(param, paramName[i])\n",
        "        gradMap.setdefault(\"%s/%s\" % (\"Gradients\", name), 0)\n",
        "        gradMap[\"%s/%s\" % (\"Gradients\", name)] += param.grad\n",
        "    return gradMap\n",
        "\n",
        "\n",
        "def logGradients(gradMap, step, scaleBy=1.0):\n",
        "    for k, v in gradMap.items():\n",
        "        experiment.log_histogram_3d(v.cpu().detach().numpy() * scaleBy, name=k, step=step)\n",
        "\n",
        "\n",
        "def logWeights(model, step):\n",
        "    for name, param in model.named_parameters():\n",
        "        paramName = name.split('.')\n",
        "        paramLabel = paramName[-1]\n",
        "        if paramLabel not in ['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0', \n",
        "                              'lowHz_', 'bandHz_', 'weight', 'bias']:\n",
        "            continue\n",
        "        param = model\n",
        "        for i in range(len(paramName)):\n",
        "            param = getattr(param, paramName[i])\n",
        "        experiment.log_histogram_3d(param.cpu().detach().numpy(), name=\"%s/%s\" % (\"Parameters\", name), step=step)\n",
        "\n",
        "\n",
        "def trainingLoop(trainDataset,\n",
        "                 valDataset,\n",
        "                 batchSize,\n",
        "                 samplingMode,\n",
        "                 cpcModel,\n",
        "                 cpcCriterion,\n",
        "                 nEpoch,\n",
        "                 optimizer,\n",
        "                 pathCheckpoint,\n",
        "                 logs,\n",
        "                 useGPU,\n",
        "                 log2Board=0):\n",
        "    print(f\"Running {nEpoch} epochs\")\n",
        "    startEpoch = len(logs[\"epoch\"])\n",
        "    bestAcc = 0\n",
        "    bestStateDict = None\n",
        "    startTime = time.time()\n",
        "    epoch = 0\n",
        "    totalSteps = 0\n",
        "    try:\n",
        "        for epoch in range(startEpoch, nEpoch):\n",
        "            print(f\"Starting epoch {epoch}\")\n",
        "            trainLoader = trainDataset.getDataLoader(batchSize, samplingMode,\n",
        "                                                    True, numWorkers=0)\n",
        "            valLoader = valDataset.getDataLoader(batchSize, samplingMode, False,\n",
        "                                                numWorkers=0)\n",
        "\n",
        "            print(\"Training dataset %d batches, Validation dataset %d batches, batch size %d\" %\n",
        "                (len(trainLoader), len(valLoader), batchSize))\n",
        "\n",
        "            locLogsTrain = trainStep(trainLoader, cpcModel, cpcCriterion, optimizer, logs[\"logging_step\"], \n",
        "                                        useGPU, log2Board, totalSteps)\n",
        "\n",
        "            totalSteps += locLogsTrain['iter']\n",
        "\n",
        "            locLogsVal = valStep(valLoader, cpcModel, cpcCriterion, useGPU)\n",
        "\n",
        "            print(f'Ran {epoch + 1} epochs '\n",
        "                f'in {time.time() - startTime:.2f} seconds')\n",
        "\n",
        "            if useGPU:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            currentAccuracy = float(locLogsVal[\"locAcc_val\"].mean())\n",
        "            \n",
        "            if log2Board:\n",
        "                for t in range(len(locLogsVal[\"locLoss_val\"])):\n",
        "                    experiment.log_metric(f\"Losses/epoch/locLoss_train_{t}\", locLogsTrain[\"locLoss_train\"][t] / totalSteps, step=epoch)\n",
        "                    experiment.log_metric(f\"Accuracy/epoch/locAcc_train_{t}\", locLogsTrain[\"locAcc_train\"][t] / totalSteps, step=epoch)\n",
        "                    experiment.log_metric(f\"Losses/epoch/locLoss_val_{t}\", locLogsVal[\"locLoss_val\"][t] / totalSteps, step=epoch)\n",
        "                    experiment.log_metric(f\"Accuracy/epoch/locAcc_val_{t}\", locLogsVal[\"locAcc_val\"][t] / totalSteps, step=epoch)\n",
        "\n",
        "            if currentAccuracy > bestAcc:\n",
        "                bestStateDict = cpcModel.state_dict()\n",
        "\n",
        "            for key, value in dict(locLogsTrain, **locLogsVal).items():\n",
        "                if key not in logs:\n",
        "                    logs[key] = [None for _ in range(epoch)]\n",
        "                if isinstance(value, np.ndarray):\n",
        "                    value = value.tolist()\n",
        "                logs[key].append(value)\n",
        "\n",
        "            logs[\"epoch\"].append(epoch)\n",
        "\n",
        "            if pathCheckpoint is not None and (epoch % logs[\"saveStep\"] == 0 or epoch == nEpoch - 1):\n",
        "                modelStateDict = cpcModel.state_dict()\n",
        "                criterionStateDict = cpcCriterion.state_dict()\n",
        "\n",
        "                save_checkpoint(modelStateDict, criterionStateDict, optimizer.state_dict(), bestStateDict,\n",
        "                                f\"{pathCheckpoint}_{epoch}.pt\")\n",
        "                save_logs(logs, pathCheckpoint + \"_logs.json\")\n",
        "    except KeyboardInterrupt:\n",
        "        if pathCheckpoint is not None:\n",
        "            modelStateDict = cpcModel.state_dict()\n",
        "            criterionStateDict = cpcCriterion.state_dict()\n",
        "\n",
        "            save_checkpoint(modelStateDict, criterionStateDict, optimizer.state_dict(), bestStateDict,\n",
        "                            f\"{pathCheckpoint}_{epoch}_interrupted.pt\")\n",
        "            save_logs(logs, pathCheckpoint + \"_logs.json\")\n",
        "        return\n",
        "\n",
        "def run(trainDataset,\n",
        "        valDataset,\n",
        "        batchSize,\n",
        "        samplingMode,\n",
        "        cpcModel,\n",
        "        cpcCriterion,\n",
        "        nEpoch,\n",
        "        optimizer,\n",
        "        pathCheckpoint,\n",
        "        logs,\n",
        "        useGPU,\n",
        "        log2Board=0):\n",
        "    if log2Board:\n",
        "        with experiment.train():\n",
        "            trainingLoop(trainDataset, valDataset, batchSize, samplingMode, cpcModel, cpcCriterion, nEpoch, optimizer,\n",
        "                         pathCheckpoint, logs, useGPU, log2Board)\n",
        "            experiment.end()\n",
        "    else:\n",
        "        trainingLoop(trainDataset, valDataset, batchSize, samplingMode, cpcModel, cpcCriterion, nEpoch, optimizer, \n",
        "                     pathCheckpoint, logs, useGPU, log2Board)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1undhM04n55"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpthiUBiVZ_W"
      },
      "source": [
        "import torch\n",
        "# from dataloader import AudioBatchData\n",
        "# from model import CPCEncoder, CPCAR, CPCModel, CPCUnsupersivedCriterion\n",
        "# from trainer import run\n",
        "from datetime import datetime\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f1IN1MRVdDy",
        "outputId": "e70ee363-4f95-47af-93f4-51b4ca9c4e44"
      },
      "source": [
        "useGPU = torch.cuda.is_available()\n",
        "labelsBy = 'ensemble'\n",
        "print(\"Loading the training dataset\")\n",
        "trainDataset = AudioBatchData(rawAudioPath='data/musicnet_lousy/train_data',\n",
        "                              metadataPath='data/musicnet_lousy/metadata_train.csv',\n",
        "                               sizeWindow=20480,\n",
        "                               labelsBy=labelsBy,\n",
        "                               outputPath='data/musicnet_lousy/train_data/train',\n",
        "                               CHUNK_SIZE=1e9,\n",
        "                               NUM_CHUNKS_INMEM=7,\n",
        "                               useGPU=useGPU)\n",
        "print(\"Training dataset loaded\")\n",
        "print(\"\")\n",
        "\n",
        "print(\"Loading the validation dataset\")\n",
        "valDataset = AudioBatchData(rawAudioPath='data/musicnet_lousy/train_data',\n",
        "                            metadataPath='data/musicnet_lousy/metadata_val.csv',\n",
        "                            sizeWindow=20480,\n",
        "                            labelsBy=labelsBy,\n",
        "                            outputPath='data/musicnet_lousy/train_data/val',\n",
        "                            CHUNK_SIZE=1e9,\n",
        "                            NUM_CHUNKS_INMEM=1,\n",
        "                            useGPU=False)\n",
        "print(\"Validation dataset loaded\")\n",
        "print(\"\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the training dataset\n",
            "Chunks already exist at data/musicnet_lousy/train_data/train/ensemble\n",
            "Loading files\n",
            "Loaded 37 sequences, elapsed=3.325 secs\n",
            "Training dataset loaded\n",
            "\n",
            "Loading the validation dataset\n",
            "Chunks already exist at data/musicnet_lousy/train_data/val/ensemble\n",
            "Loading files\n",
            "Loaded 32 sequences, elapsed=9.170 secs\n",
            "Validation dataset loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsL80pscYyQs",
        "outputId": "b61b7b8e-fec1-4950-9015-ff39414074d9"
      },
      "source": [
        "# # valLoader = valDataset.getDataLoader(6, 'samecategory', False, numWorkers=0)\n",
        "# trainLoader = trainDataset.getDataLoader(6, 'samecategory', False, numWorkers=0)\n",
        "\n",
        "# for step, fulldata in enumerate(trainLoader):\n",
        "#     # print(fulldata[0].shape)\n",
        "#     # print(fulldata[1])\n",
        "#     pass"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading files\n",
            "Loaded 45 sequences, elapsed=3.211 secs\n",
            "Loading files\n",
            "Loaded 44 sequences, elapsed=20.685 secs\n",
            "Loading files\n",
            "Loaded 46 sequences, elapsed=20.502 secs\n",
            "Loading files\n",
            "Loaded 44 sequences, elapsed=19.665 secs\n",
            "Loading files\n",
            "Loaded 35 sequences, elapsed=20.117 secs\n",
            "Loading files\n",
            "Loaded 37 sequences, elapsed=18.161 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBri49jRXtf0",
        "outputId": "31d10b8e-4c62-4687-9df7-20ba6894ddfe"
      },
      "source": [
        "useTransformer = True\n",
        "samplingType = 'samecategory'\n",
        "\n",
        "# Encoder network\n",
        "encoderNet = CPCEncoder(512, 'layerNorm', sincNet=True)\n",
        "# AR Network\n",
        "if useTransformer:\n",
        "    arNet = buildTransformerAR(512, 1, 20480 // 160, abspos=False)\n",
        "    hiddenGAr = 512\n",
        "else:\n",
        "    arNet = CPCAR(512, 256, samplingType == 'sequential', 1, mode=\"GRU\", reverse=False)\n",
        "    hiddenGAr = 256\n",
        "\n",
        "cpcModel = CPCModel(encoderNet, arNet)\n",
        "batchSize = 8\n",
        "cpcModel.supervised = False\n",
        "\n",
        "cpcCriterion = CPCUnsupersivedCriterion(nPredicts=12,\n",
        "                                        dimOutputAR=hiddenGAr,\n",
        "                                        dimOutputEncoder=512,\n",
        "                                        negativeSamplingExt=128,\n",
        "                                        mode=None,\n",
        "                                        dropout=False)\n",
        "\n",
        "if useGPU:\n",
        "    cpcCriterion.cuda()\n",
        "    cpcModel.cuda()\n",
        "\n",
        "gParams = list(cpcCriterion.parameters()) + list(cpcModel.parameters())\n",
        "lr = 2e-4\n",
        "optimizer = torch.optim.Adam(gParams, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "\n",
        "expDescription = f'{samplingType}_'\n",
        "if samplingType == 'samecategory':\n",
        "    expDescription += f'{labelsBy}_'\n",
        "\n",
        "pathCheckpoint = f'logs/{expDescription}{datetime.now().strftime(\"%d-%m_%H-%M-%S\")}'\n",
        "os.makedirs(pathCheckpoint, exist_ok=True)\n",
        "pathCheckpoint = os.path.join(pathCheckpoint, \"checkpoint\")\n",
        "\n",
        "logs = {\"epoch\": [], \"iter\": [], \"saveStep\": 1, \"logging_step\": 1000}\n",
        "\n",
        "run(trainDataset, valDataset, batchSize, samplingType, cpcModel, cpcCriterion, 30, optimizer, pathCheckpoint, logs, \n",
        "    useGPU, log2Board=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 30 epochs\n",
            "Starting epoch 0\n",
            "Training dataset 10681 batches, Validation dataset 1172 batches, batch size 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.ml/tiagocuervo/jtm/1460522bb7bf42828c0363422d54dcbf\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_0 [23]  : (0.008620689623057842, 0.1904388734732162)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_1 [23]  : (0.004310344811528921, 0.04649014840833843)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_10 [23] : (0.005387931130826473, 0.0279498920426704)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_11 [23] : (0.00754310330376029, 0.02270628062875143)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_2 [23]  : (0.008620689390227199, 0.03394396521616727)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_3 [23]  : (0.006465517450124025, 0.04642600585551312)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_4 [23]  : (0.008081896463409066, 0.05702227031967292)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_5 [23]  : (0.006465517450124025, 0.05056958146659391)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_6 [23]  : (0.00754310330376029, 0.032096674699070196)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_7 [23]  : (0.009159482549875975, 0.02909482743901511)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_8 [23]  : (0.005387931130826473, 0.023635057204713424)\n",
            "COMET INFO:     train_Accuracy/batch/locAcc_train_9 [23]  : (0.008620689623057842, 0.027837643244614203)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_0 [23]   : (4.7614426612854, 4.859411716461182)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_1 [23]   : (4.795511535976244, 4.85935115814209)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_10 [23]  : (4.8193718661432685, 4.859849452972412)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_11 [23]  : (4.821714235388714, 4.859687805175781)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_2 [23]   : (4.803744005120319, 4.859500885009766)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_3 [23]   : (4.803141552469005, 4.859984397888184)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_4 [23]   : (4.803033393362294, 4.859515190124512)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_5 [23]   : (4.80214087859444, 4.859481334686279)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_6 [23]   : (4.80535252197929, 4.860020160675049)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_7 [23]   : (4.810589002526325, 4.859917163848877)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_8 [23]   : (4.811076661814814, 4.859992027282715)\n",
            "COMET INFO:     train_Losses/batch/locLoss_train_9 [23]   : (4.811063310374385, 4.85980749130249)\n",
            "COMET INFO:     train_loss [2]                            : (56.761749267578125, 58.00815963745117)\n",
            "COMET INFO:   Uploads [count]:\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     histogram3d [44]    : 44\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 1\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
            "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5WYUmPQExz4"
      },
      "source": [
        "# gru = nn.GRU(512, 256, num_layers=1, batch_first=True)\n",
        "# sizeHidden = 512\n",
        "# sinc0 = SincConv1D(1, sizeHidden, 10, stride=5, padding=3)\n",
        "# conv0 = nn.Conv1d(1, sizeHidden, 10, stride=5, padding=3)\n",
        "# batchNorm0 = ChannelNorm(sizeHidden)\n",
        "# reluLayer = nn.LeakyReLU(0.1)\n",
        "# print(\"GRU\")\n",
        "# for name, param in gru.named_parameters():\n",
        "#     print(\"\\t\", name)\n",
        "#     print(\"\\t\", name.split('.')[-1])\n",
        "# print(\"Sinc\")\n",
        "# for name, param in sinc0.named_parameters():\n",
        "#     print(\"\\t\", name)\n",
        "#     print(\"\\t\", name.split('.')[-1])\n",
        "# print(\"Conv\")\n",
        "# for name, param in conv0.named_parameters():\n",
        "#     print(\"\\t\", name)\n",
        "#     print(\"\\t\", name.split('.')[-1])\n",
        "# print(\"BatchNorm\")\n",
        "# for name, param in batchNorm0.named_parameters():\n",
        "#     print(\"\\t\", name)\n",
        "#     print(\"\\t\", name.split('.')[-1])\n",
        "# print(\"Transformer\")\n",
        "# for name, param in arNet.named_parameters():\n",
        "#     print(\"\\t\", name)\n",
        "#     print(\"\\t\", name.split('.')[-1])\n",
        "# print(\"ReLU\")\n",
        "# for name, param in reluLayer.named_parameters():\n",
        "#     print(\"\\t\", name)\n",
        "#     print(\"\\t\", name.split('.')[-1])\n",
        "# print(len(list(reluLayer.named_parameters())))"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}